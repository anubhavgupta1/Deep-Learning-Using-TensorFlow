{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3ZPT0Uh2U4S6jUlmXboww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Deep-Learning-Using-TensorFlow/blob/master/2_Training_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC6jCBi0eBOG"
      },
      "source": [
        "#**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLAs1yE64kZh",
        "outputId": "55b9d2c0-f028-4d2a-9ed0-58562449f68b"
      },
      "source": [
        "!ls\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/\")\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.23-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "'Capstone Project Submission .desktop'\t\t     model_ckps\n",
            "'Capstone Project Submission  (Responses).desktop'   model_ckps1\n",
            "'Colab Notebooks'\t\t\t\t     model_ckps2\n",
            " data\t\t\t\t\t\t     model_ckps3\n",
            " Dataset\t\t\t\t\t     model_ckps4\n",
            "'Getting started'\t\t\t\t     model_ckps5\n",
            " images\t\t\t\t\t\t     model_ckps6\n",
            " import\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NGp9k_-EHjsJ",
        "outputId": "75b9132f-be70-458f-f7e7-435697027306"
      },
      "source": [
        "!pip install tensorflow==1.10.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/e6/a6d371306c23c2b01cd2cb38909673d17ddd388d9e4b3c0f6602bfd972c8/tensorflow-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4MB 72kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.33.2)\n",
            "Collecting numpy<=1.14.5,>=1.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.35.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.3.3)\n",
            "Collecting tensorboard<1.11.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 42.0MB/s \n",
            "\u001b[?25hCollecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.4.0)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.4 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.17.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.1 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorboard, setuptools, tensorflow\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed numpy-1.14.5 setuptools-39.1.0 tensorboard-1.10.0 tensorflow-1.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqLgCrtT4qOX",
        "outputId": "a261af42-f0bf-40a3-c294-595a3d3ca799"
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from matplotlib.colors import ListedColormap\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8T4M6N94xpT"
      },
      "source": [
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deep\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQfDEFLkXs_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ce97f7-1f81-4efc-cf58-626f9351f31b"
      },
      "source": [
        "mnist = input_data.read_data_sets(\"data/\")\n",
        "X_train = mnist.train.images\n",
        "X_test = mnist.test.images\n",
        "y_train = mnist.train.labels.astype(\"int\")\n",
        "y_test = mnist.test.labels.astype(\"int\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-36e6f08442f0>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FGFgBQMY9SV"
      },
      "source": [
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_hidden5 = 50\n",
        "n_outputs = 10"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwnAv7gBZEdI"
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we5_pD2qnwGu"
      },
      "source": [
        "What!? That's not a great accuracy for MNIST. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets.\n",
        "\n",
        "One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_Y185QKn1pg",
        "outputId": "f8b510ce-214e-4d85-87b2-7725f16d26a9"
      },
      "source": [
        "# List trainable variables\n",
        "\n",
        "[v.name for v in tf.trainable_variables()]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ytJ_Ven5ye",
        "outputId": "c5936857-2ac1-45d4-eddd-d23255460839"
      },
      "source": [
        "# List global variables\n",
        "\n",
        "[v.name for v in tf.global_variables()]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL-Uz2amnwiX"
      },
      "source": [
        "#**Partial Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqth8GJ1n0c_"
      },
      "source": [
        "def multiply(x, y):\n",
        "    return x*y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpUya1PtolXN"
      },
      "source": [
        "def multiplyByTwo_1(x):\n",
        "    return multiply(x, 2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObuOG-hboC9o"
      },
      "source": [
        "multiplyByTwo = partial(multiply, 2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x57PhXARorQ3",
        "outputId": "443c139e-9717-4f7d-ba74-4932eb99a07a"
      },
      "source": [
        "multiplyByTwo_1(4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjwxHM-RouyC",
        "outputId": "c5016b6c-c0c0-4ca3-b2ed-d1966418cdda"
      },
      "source": [
        "multiplyByTwo(4)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esJEHtozr6tG"
      },
      "source": [
        "#**Dummy Example of Standardisation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzPQGXYGYeHT",
        "outputId": "fbb5d168-4cc3-4e85-bd94-3f6f1f92e1a8"
      },
      "source": [
        "x = np.array([0.1,0.01,1,100,200,400,50000,100000])\n",
        "mean  = np.mean(x)\n",
        "mean"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18837.63875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIAl1gbpZZfs",
        "outputId": "0aea8887-c0ea-444b-ee7b-45b4b8787633"
      },
      "source": [
        "dx = x - mean\n",
        "dx"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-18837.53875, -18837.62875, -18836.63875, -18737.63875,\n",
              "       -18637.63875, -18437.63875,  31162.36125,  81162.36125])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpGCc3mpaCmA",
        "outputId": "9ab1edbe-c25a-48b0-efcb-ad10d7257f92"
      },
      "source": [
        "std = np.sum(dx*dx)/len(x)\n",
        "std"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1207669616.4507608"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6_dCyR2afkc",
        "outputId": "3b89e161-5267-4d31-d270-20394c1e05d1"
      },
      "source": [
        "x_scaled = dx / np.sqrt(std)\n",
        "x_scaled"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.5420634 , -0.54206599, -0.54203751, -0.53918871, -0.53631114,\n",
              "       -0.530556  ,  0.89671883,  2.33550394])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHxhAVPimbkw"
      },
      "source": [
        "#**Dummy Example of Gradient clipping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkMBIYGAbB3u"
      },
      "source": [
        "a = [(3.4,\"a1\"),(1.8,\"a2\"),(-2.9,\"a3\"),(-3.1,\"a4\")]\n",
        "#grads_and_vars = optimizer.compute_gradients(loss)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMiiSam2bmam"
      },
      "source": [
        "def clip_by_value(x , th):\n",
        "  if x > th :\n",
        "    return th\n",
        "  if x < -th :\n",
        "    return -th\n",
        "  return x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgwfZ_rEjCkb",
        "outputId": "cb601815-6b3f-447a-9675-210d8b97c451"
      },
      "source": [
        "b = []\n",
        "th = 3\n",
        "for k , v in a :\n",
        "  b.append((clip_by_value(k,th), v))\n",
        "  #capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
        "b"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 'a1'), (1.8, 'a2'), (-2.9, 'a3'), (-3, 'a4')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siOCezVAlECx",
        "outputId": "a8d1f56d-af21-4b54-98d0-3edade58412a"
      },
      "source": [
        "[(clip_by_value(k,th), v) for k , v in a]\n",
        "#capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 'a1'), (1.8, 'a2'), (-2.9, 'a3'), (-3, 'a4')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogf8FdYivbM0"
      },
      "source": [
        "#**Gradient Clipping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFIwX8lov_Gw"
      },
      "source": [
        "he_init = tf.contrib.layers.variance_scaling_initializer()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdV3afGIvSuC"
      },
      "source": [
        "# Create a simple neural net for MNIST and add gradient clipping\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.selu, name=\"hidden1\",kernel_initializer=he_init)\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.selu, name=\"hidden2\",kernel_initializer=he_init)\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.selu, name=\"hidden3\",kernel_initializer=he_init)\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.selu, name=\"hidden4\",kernel_initializer=he_init)\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.selu, name=\"hidden5\",kernel_initializer=he_init)\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-GO253MwYEV"
      },
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCpoaBMkwfuI"
      },
      "source": [
        "learning_rate = 0.01\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MbC9V7Wxwci"
      },
      "source": [
        "# Now we apply gradient clipping.\n",
        "# For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them\n",
        "\n",
        "threshold = 1.0\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
        "training_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu5FBQDCwoXP"
      },
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1) #It says whether the targets are in top k predictions\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dFXZaK_ws_D"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBP74zdHwyPj"
      },
      "source": [
        "n_epochs = 40\n",
        "batch_size = 200"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SgdYzzsw00G",
        "outputId": "10b2db91-4f59-4a41-8057-d306b20c9b52"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,y: mnist.test.labels})\n",
        "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"model_ckps7/my_model_final.ckpt\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Test accuracy: 0.8988\n",
            "1 Test accuracy: 0.9211\n",
            "2 Test accuracy: 0.9282\n",
            "3 Test accuracy: 0.9366\n",
            "4 Test accuracy: 0.9409\n",
            "5 Test accuracy: 0.9428\n",
            "6 Test accuracy: 0.9464\n",
            "7 Test accuracy: 0.9486\n",
            "8 Test accuracy: 0.9494\n",
            "9 Test accuracy: 0.9523\n",
            "10 Test accuracy: 0.9539\n",
            "11 Test accuracy: 0.956\n",
            "12 Test accuracy: 0.9562\n",
            "13 Test accuracy: 0.9573\n",
            "14 Test accuracy: 0.9574\n",
            "15 Test accuracy: 0.9589\n",
            "16 Test accuracy: 0.9593\n",
            "17 Test accuracy: 0.9589\n",
            "18 Test accuracy: 0.9601\n",
            "19 Test accuracy: 0.9625\n",
            "20 Test accuracy: 0.9608\n",
            "21 Test accuracy: 0.962\n",
            "22 Test accuracy: 0.9626\n",
            "23 Test accuracy: 0.9642\n",
            "24 Test accuracy: 0.9636\n",
            "25 Test accuracy: 0.9646\n",
            "26 Test accuracy: 0.9642\n",
            "27 Test accuracy: 0.9643\n",
            "28 Test accuracy: 0.965\n",
            "29 Test accuracy: 0.9659\n",
            "30 Test accuracy: 0.9674\n",
            "31 Test accuracy: 0.9664\n",
            "32 Test accuracy: 0.965\n",
            "33 Test accuracy: 0.9672\n",
            "34 Test accuracy: 0.9679\n",
            "35 Test accuracy: 0.9675\n",
            "36 Test accuracy: 0.9676\n",
            "37 Test accuracy: 0.9681\n",
            "38 Test accuracy: 0.9676\n",
            "39 Test accuracy: 0.9675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwynBWRPw6ww",
        "outputId": "204f1ec4-7aab-4ebf-8344-de26c9359c96"
      },
      "source": [
        "!ls -l model_ckps7/my_model_final.ckpt*"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 1034840 Dec  6 15:35 model_ckps7/my_model_final.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root     487 Dec  6 15:35 model_ckps7/my_model_final.ckpt.index\n",
            "-rw-r--r-- 1 root root   54086 Dec  6 15:35 model_ckps7/my_model_final.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
