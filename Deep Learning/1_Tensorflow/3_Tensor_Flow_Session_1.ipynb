{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Tensor_Flow_Session_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN721qrA2QEUehFiKQKLgYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Deep-Learning-Using-TensorFlow/blob/master/3_Tensor_Flow_Session_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_YmUb8dH7we",
        "outputId": "5e7e05b5-2a73-4c5d-c700-a77c3d55e73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==1.10.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/e6/a6d371306c23c2b01cd2cb38909673d17ddd388d9e4b3c0f6602bfd972c8/tensorflow-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4MB 76kB/s \n",
            "\u001b[?25hCollecting numpy<=1.14.5,>=1.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 20.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.10.0)\n",
            "Collecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (3.12.4)\n",
            "Collecting tensorboard<1.11.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.33.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.4.0)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.4 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.17.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.1 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, setuptools, tensorboard, tensorflow\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed numpy-1.14.5 setuptools-39.1.0 tensorboard-1.10.0 tensorflow-1.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YYcViNXGBNu",
        "outputId": "1cc7a4ad-de2d-4357-e3b6-225744e4b1ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SwP1a9JuTff"
      },
      "source": [
        "x = 1\n",
        "y = 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWKmAaYaPDWK",
        "outputId": "48831c61-f7d0-41a0-c8be-bd437098fc1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f = x*x*y + y + 2\n",
        "f"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-coXccTfPHnE",
        "outputId": "bae9356b-0ea7-4ca4-97a8-18fb1109b96a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoW6jYpFPMBn"
      },
      "source": [
        "x = 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ27XQuMPR2C",
        "outputId": "b5678dcf-fc11-4c09-b1a0-1a7bb988f376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f = x*x*y + y + 2\n",
        "f"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhdgOg5KPc-k"
      },
      "source": [
        "def differentiate(f, x):\n",
        "    dx = 0.00001\n",
        "    return (f(x+dx) - f(x))/dx"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY62dWAuPg3-"
      },
      "source": [
        "def sq(x):\n",
        "    return x*x\n",
        "\n",
        "def cube(x):\n",
        "    return x*x*x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3pVSBVpPi7t",
        "outputId": "8247b226-bbab-4785-d08e-a7ed19f43d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "differentiate(sq, 5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.000009999444615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtvTZy42Qwpz",
        "outputId": "9976aa46-1fdb-4d8c-e0e5-7147f7f7b85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "differentiate(cube, 5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75.00014999664018"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_b5TmivRHbZ"
      },
      "source": [
        "def fn(x,y):\n",
        "  return x*x*y + y + 2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXRJl7482h_e",
        "outputId": "ea092bbc-9d8b-4176-f18c-47f8b80f9b5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = np.array([[1, 5/3], [1, 1/9]])\n",
        "y = np.array([12/3, 21/9])\n",
        "np.linalg.inv(X.T.dot(X)) .dot(X.T).dot(y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.21428571, 1.07142857])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-LtqnYdGswX"
      },
      "source": [
        "###**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtNQX0MrA6H0",
        "outputId": "efbd400b-a706-41bb-8ad6-07fed06cc8fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the tensorflow version\n",
        "print(tf.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECsohbi-HFyl"
      },
      "source": [
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"tensorflow\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laBHOR8xHaxG"
      },
      "source": [
        "###**Creating and running a graph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaSrpcrpHQYc"
      },
      "source": [
        "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "root_logdir = \"tf_logs\"\n",
        "logdir = \"{}/run-{}/\".format(root_logdir, now)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02-gH1rmHgmK"
      },
      "source": [
        "reset_graph()\n",
        "x = tf.Variable(3, name=\"x\")\n",
        "y = tf.Variable(4, name=\"y\")\n",
        "f = x*x*y + y + 2\n",
        "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxkUYr3UHmy4"
      },
      "source": [
        "g = tf.get_default_graph()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S84nf9XjHp20"
      },
      "source": [
        "file_writer.close()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ksAFOZqQiPg",
        "outputId": "f941420d-1d94-4d05-ef36-a81a434ea7db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(x.initializer)\n",
        "sess.run(y.initializer)\n",
        "result = sess.run(f)\n",
        "print(result)\n",
        "sess.close()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SdY2ZEHQkpp",
        "outputId": "7874f82f-8532-4e9a-a692-8cca61b7f2b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Better way\n",
        "with tf.Session() as sess:\n",
        "    x.initializer.run()\n",
        "    y.initializer.run()\n",
        "    result = sess.run(f)\n",
        "    print(result)\n",
        "result"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKfIkvIAQsc0",
        "outputId": "3c3fc461-ac6a-4c42-a3d2-78bf628b219c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Better way\n",
        "with tf.Session() as sess:\n",
        "    x.initializer.run()\n",
        "    y.initializer.run()\n",
        "    result = f.eval()\n",
        "    print(result)\n",
        "result"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgQCXafbQwO5",
        "outputId": "cd18fc9a-a4a1-4687-ce96-b3bede463ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Another better way\n",
        "\n",
        "init = tf.global_variables_initializer() # prepare an init node\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run() # actually initialize all the variables\n",
        "    result = f.eval()\n",
        "\n",
        "result"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gXUCu1IQyO7",
        "outputId": "5e98530d-dc30-4588-90d2-12a2b27d9e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Another better way\n",
        "\n",
        "init = tf.global_variables_initializer() # prepare an init node\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run() # actually initialize all the variables\n",
        "    print(f.eval())\n",
        "    print(sess.run(f))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n",
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_CDUPxSQ3vh"
      },
      "source": [
        "###**Managing graphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVMdVK74Q2ZD"
      },
      "source": [
        "reset_graph()\n",
        "x1 = tf.Variable(1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5CROFzgRCMq",
        "outputId": "35464109-f9be-4fd6-a1d3-eceb68350d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.get_default_graph()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.framework.ops.Graph at 0x7f436b266b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh-gA2vIRHxB",
        "outputId": "8c70c39f-04b8-42ff-8fa2-0f08e194dab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x1.graph is tf.get_default_graph()\n",
        "print(tf.get_default_graph())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.framework.ops.Graph object at 0x7f436b266b70>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gakIXBgIRLoO",
        "outputId": "9d264912-6d1b-49dd-cbd6-cbbce2febad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "graph1 = tf.Graph()\n",
        "\n",
        "with graph1.as_default():\n",
        "    x2 = tf.Variable(2)\n",
        "\n",
        "print(x2.graph is graph1)\n",
        "print(x2.graph is tf.get_default_graph())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg5DCQSlRNCO"
      },
      "source": [
        "# To reset the default group\n",
        "reset_graph()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYqAImoGRT7l"
      },
      "source": [
        "###**Lifecycle of a Node Value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkGxRfSvRW3s",
        "outputId": "59e1b9cf-f5d5-49ac-a555-73418dd690af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "graph1.get_collection('variables')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'Variable:0' shape=() dtype=int32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7e5H1xlRZaA",
        "outputId": "ce9bc1a9-16a5-480a-b497-6be2a02e0211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "g = tf.get_default_graph()\n",
        "g.get_operations()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBPEL_bJReyJ"
      },
      "source": [
        "npa = np.array([1,2,3])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zECHQrGLRjAM"
      },
      "source": [
        "init = tf.global_variables_initializer() # prepare an init node\n",
        "\n",
        "with tf.Session() as s:\n",
        "    init.run()\n",
        "    #print(g.get_operation_by_name('s'))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IsdGiTURj9e",
        "outputId": "9f16ba7d-62bd-455e-a603-7e25757aa56c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w = tf.constant(3)\n",
        "x = w + 2\n",
        "y = x + 5\n",
        "z = x * 3\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(y.eval())  # 10\n",
        "    print(z.eval())  # 15"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoIwUmBqRoel",
        "outputId": "6108cb23-96f2-4334-944c-0cb20d25f969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "zz = tf.square(y+z)\n",
        "with tf.Session() as s:\n",
        "    zz_v, z_v, y_v = s.run([zz, z, y])\n",
        "print(zz_v)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdKUJAgBRuRV",
        "outputId": "00c184f9-ce3c-4e6a-8941-e2112a0f75cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Run previous code efficiently\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    y_val, z_val = sess.run([y, z])\n",
        "    print(y_val)  # 10\n",
        "    print(z_val)  # 15"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2qqKg0uR5cj"
      },
      "source": [
        "###**Linear Regression Using numpy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gptpafj8YwfO",
        "outputId": "f5102e5b-e72e-455f-fe27-4fa79eba8df3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = np.array([[1,5/3],[1, 1/9]])\n",
        "y = np.array([[12/3],[21/9]])\n",
        "\n",
        "# (XT.X)-1  .XT   .y\n",
        "xt = X.T\n",
        "xtx = xt.dot(X)\n",
        "ixtx =  np.linalg.inv(xtx)\n",
        "\n",
        "theta_best = ixtx.dot(xt).dot(y)\n",
        "theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "theta_best"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.21428571],\n",
              "       [1.07142857]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7iS5gBZa2Jd"
      },
      "source": [
        "###**Using the Normal Equation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8lXF_SIZBWM"
      },
      "source": [
        "reset_graph()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DXGoxG_ZY9K",
        "outputId": "07a9dab9-c5ff-4f1a-93b6-85a1dc9cd266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array([1, 2, 3]) - np.array([3, 2, 1])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2,  0,  2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxlOHj-ZZeKQ",
        "outputId": "9e0efce5-5251-4259-f72e-0cee0e4a9962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "housing = fetch_california_housing()\n",
        "m, n = housing.data.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhksSaXGZh8y",
        "outputId": "bcc3388f-b31a-45e5-b3ae-c64297f970eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "housing.data[0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   8.3252    ,   41.        ,    6.98412698,    1.02380952,\n",
              "        322.        ,    2.55555556,   37.88      , -122.23      ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWv3gXqLZllp",
        "outputId": "d14c005b-d977-4ee8-866e-c47479aae3e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "housing.target[0]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2ryZRkkZpIV"
      },
      "source": [
        "mydata = np.array([[2, 3], [5,6]])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEdS4XYQZsWI",
        "outputId": "c8c9c952-197b-4670-c1ce-0d288dfa50cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.ones((2, 3))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1.],\n",
              "       [1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCEZ-rdoZv2T",
        "outputId": "3e107255-5c40-48cd-bb2b-379eb05e537e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mydata"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 3],\n",
              "       [5, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfpBZC1AZy2c",
        "outputId": "e3d3e69e-fc36-4f42-daae-1acdb6ae7bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.c_[np.ones((2, 1)), mydata]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 3.],\n",
              "       [1., 5., 6.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ez2Kkn2Z2eT"
      },
      "source": [
        "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YcLJdTrZ6Ba",
        "outputId": "ee6c8f3e-4875-4e53-f57f-febc093ea4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "v = np.array([1,2,3,4, 6, 7])\n",
        "v"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 6, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SblBWsWtZ-77",
        "outputId": "f6cb92ac-9ebb-486c-a6f0-52d65f2a92f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "v1 = v.reshape((-1, 2))\n",
        "v1"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4],\n",
              "       [6, 7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGtQANjNaDNP",
        "outputId": "51d4c88f-f11f-4a3a-932f-36418250bd52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "v1.T"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 3, 6],\n",
              "       [2, 4, 7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz9ZorIlaJYr"
      },
      "source": [
        "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
        "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBNW7ToHaPdR",
        "outputId": "c0468c30-e66c-4741-ed4e-b19ffc5b9c18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate theta using normal equation in TensorFlow\n",
        "# We will use housing dataset of end-to-end project\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "m, n = housing.data.shape\n",
        "\n",
        "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
        "\n",
        "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
        "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
        "\n",
        "# (XT.X)-1  .XT   .y\n",
        "\n",
        "XT = tf.transpose(X)\n",
        "theta = tf.matmul(\n",
        "    tf.matmul(\n",
        "        tf.matrix_inverse(\n",
        "            tf.matmul(\n",
        "                XT, \n",
        "                X)), \n",
        "        XT), \n",
        "    y)\n",
        "\n",
        "# theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul( tf.transpose(X), X)),  \n",
        "#                             tf.transpose(X)), y)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    theta_value = theta.eval()\n",
        "theta_value"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.7185181e+01],\n",
              "       [ 4.3633747e-01],\n",
              "       [ 9.3952334e-03],\n",
              "       [-1.0711310e-01],\n",
              "       [ 6.4479220e-01],\n",
              "       [-4.0338000e-06],\n",
              "       [-3.7813708e-03],\n",
              "       [-4.2348403e-01],\n",
              "       [-4.3721911e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJem3RHjacqq",
        "outputId": "19392582-7b4f-4328-ef20-496c88f58a31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "housing.target"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EGGlPwMafUy",
        "outputId": "e57174c1-5ccc-44b4-d086-c633e664c71e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "housing.target.reshape(-1, 1)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.526],\n",
              "       [3.585],\n",
              "       [3.521],\n",
              "       ...,\n",
              "       [0.923],\n",
              "       [0.847],\n",
              "       [0.894]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WewfmLcDak3m",
        "outputId": "1898593d-41f0-46e6-afdf-c1291f97f3ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compare it with pure NumPy\n",
        "# Calculate theta using normal equation in NumPy\n",
        "\n",
        "X = housing_data_plus_bias\n",
        "y = housing.target.reshape(-1, 1)\n",
        "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "print(theta_numpy)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.69419202e+01]\n",
            " [ 4.36693293e-01]\n",
            " [ 9.43577803e-03]\n",
            " [-1.07322041e-01]\n",
            " [ 6.45065694e-01]\n",
            " [-3.97638942e-06]\n",
            " [-3.78654265e-03]\n",
            " [-4.21314378e-01]\n",
            " [-4.34513755e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiS7F_dWaonQ",
        "outputId": "0899e819-0d63-4ee8-caa7-adfe7e64ee3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compare it with pure NumPy\n",
        "# Calculate theta using Scikit-Learn\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
        "\n",
        "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.69419202e+01]\n",
            " [ 4.36693293e-01]\n",
            " [ 9.43577803e-03]\n",
            " [-1.07322041e-01]\n",
            " [ 6.45065694e-01]\n",
            " [-3.97638942e-06]\n",
            " [-3.78654265e-03]\n",
            " [-4.21314378e-01]\n",
            " [-4.34513755e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4it8HPybsOt"
      },
      "source": [
        "###**Using Batch Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKd_IPjxbqjs",
        "outputId": "4b767df7-4628-4edd-d3d9-481ad1dfc2df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Gradient Descent requires scaling the feature vectors first\n",
        "# We could do this using TF, but let's just use Scikit-Learn for now.\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_housing_data = scaler.fit_transform(housing.data)\n",
        "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
        "\n",
        "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
        "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
        "print(scaled_housing_data_plus_bias.mean())\n",
        "print(scaled_housing_data_plus_bias.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
            " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
            " -8.52651283e-15]\n",
            "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
            "  0.01359031]\n",
            "0.11111111111111005\n",
            "(20640, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwYxzPsGb4Pp",
        "outputId": "e46cc072-cab6-49ea-b332-5976b1d86bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "scaled_housing_data_plus_bias.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uml_T-P8b5wi",
        "outputId": "7dac0ec7-ef10-422a-99e7-097fe7cdfd17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN6AH6smcCo0",
        "outputId": "69d2324f-e02b-44bd-aa65-5f551c5bb4a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Step 2- Manually computing the gradient\n",
        "# The Code is self-explanatory, except for a few new elements\n",
        "\n",
        "'''\n",
        "\n",
        "1. random_uniform() - The random_uniform() function creates a node in the graph that will generate a tensor containing\n",
        "random values, given its shape and value range, much like NumPy’s rand() function.\n",
        "\n",
        "2. assign() - The assign() function creates a node that will assign a new value to a variable. In this case, it\n",
        "implements the Batch Gradient Descent step θ(next step) = θ – η∇θMSE(θ).\n",
        "\n",
        "3. The main loop executes the training step over and over again (n_epochs times), and every 100 iterations it prints \n",
        "out the current Mean Squared Error (mse). We can see the MSE go down at every iteration\n",
        "\n",
        "'''\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_epochs = 10000\n",
        "learning_rate = .1\n",
        "\n",
        "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
        "m, n = housing.data.shape\n",
        "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
        "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
        "\n",
        "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
        "error = y_pred - y\n",
        "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
        "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
        "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(n_epochs):\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
        "        x = sess.run(training_op)\n",
        "    best_theta = theta.eval()\n",
        "print(best_theta)\n",
        "print(x)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 MSE = 9.161543\n",
            "Epoch 10 MSE = 0.66590255\n",
            "Epoch 20 MSE = 0.5652157\n",
            "Epoch 30 MSE = 0.5554022\n",
            "Epoch 40 MSE = 0.5486719\n",
            "Epoch 50 MSE = 0.54350525\n",
            "Epoch 60 MSE = 0.53950936\n",
            "Epoch 70 MSE = 0.5364025\n",
            "Epoch 80 MSE = 0.53397423\n",
            "Epoch 90 MSE = 0.53206617\n",
            "Epoch 100 MSE = 0.5305595\n",
            "Epoch 110 MSE = 0.5293639\n",
            "Epoch 120 MSE = 0.528411\n",
            "Epoch 130 MSE = 0.52764815\n",
            "Epoch 140 MSE = 0.52703446\n",
            "Epoch 150 MSE = 0.52653974\n",
            "Epoch 160 MSE = 0.5261395\n",
            "Epoch 170 MSE = 0.5258139\n",
            "Epoch 180 MSE = 0.52554905\n",
            "Epoch 190 MSE = 0.5253327\n",
            "Epoch 200 MSE = 0.52515554\n",
            "Epoch 210 MSE = 0.52501005\n",
            "Epoch 220 MSE = 0.5248907\n",
            "Epoch 230 MSE = 0.5247927\n",
            "Epoch 240 MSE = 0.5247114\n",
            "Epoch 250 MSE = 0.5246444\n",
            "Epoch 260 MSE = 0.52458924\n",
            "Epoch 270 MSE = 0.5245437\n",
            "Epoch 280 MSE = 0.52450603\n",
            "Epoch 290 MSE = 0.5244742\n",
            "Epoch 300 MSE = 0.5244485\n",
            "Epoch 310 MSE = 0.5244268\n",
            "Epoch 320 MSE = 0.52440894\n",
            "Epoch 330 MSE = 0.5243941\n",
            "Epoch 340 MSE = 0.52438176\n",
            "Epoch 350 MSE = 0.52437174\n",
            "Epoch 360 MSE = 0.52436316\n",
            "Epoch 370 MSE = 0.52435607\n",
            "Epoch 380 MSE = 0.5243503\n",
            "Epoch 390 MSE = 0.5243454\n",
            "Epoch 400 MSE = 0.52434105\n",
            "Epoch 410 MSE = 0.52433777\n",
            "Epoch 420 MSE = 0.524335\n",
            "Epoch 430 MSE = 0.52433264\n",
            "Epoch 440 MSE = 0.5243306\n",
            "Epoch 450 MSE = 0.52432907\n",
            "Epoch 460 MSE = 0.5243277\n",
            "Epoch 470 MSE = 0.5243265\n",
            "Epoch 480 MSE = 0.52432567\n",
            "Epoch 490 MSE = 0.52432466\n",
            "Epoch 500 MSE = 0.52432406\n",
            "Epoch 510 MSE = 0.5243236\n",
            "Epoch 520 MSE = 0.52432305\n",
            "Epoch 530 MSE = 0.5243227\n",
            "Epoch 540 MSE = 0.52432245\n",
            "Epoch 550 MSE = 0.52432233\n",
            "Epoch 560 MSE = 0.5243218\n",
            "Epoch 570 MSE = 0.52432173\n",
            "Epoch 580 MSE = 0.5243216\n",
            "Epoch 590 MSE = 0.52432126\n",
            "Epoch 600 MSE = 0.52432144\n",
            "Epoch 610 MSE = 0.5243213\n",
            "Epoch 620 MSE = 0.52432126\n",
            "Epoch 630 MSE = 0.524321\n",
            "Epoch 640 MSE = 0.52432096\n",
            "Epoch 650 MSE = 0.5243211\n",
            "Epoch 660 MSE = 0.524321\n",
            "Epoch 670 MSE = 0.524321\n",
            "Epoch 680 MSE = 0.5243212\n",
            "Epoch 690 MSE = 0.52432114\n",
            "Epoch 700 MSE = 0.52432096\n",
            "Epoch 710 MSE = 0.52432096\n",
            "Epoch 720 MSE = 0.524321\n",
            "Epoch 730 MSE = 0.52432084\n",
            "Epoch 740 MSE = 0.52432084\n",
            "Epoch 750 MSE = 0.52432096\n",
            "Epoch 760 MSE = 0.52432096\n",
            "Epoch 770 MSE = 0.5243211\n",
            "Epoch 780 MSE = 0.52432084\n",
            "Epoch 790 MSE = 0.524321\n",
            "Epoch 800 MSE = 0.5243211\n",
            "Epoch 810 MSE = 0.524321\n",
            "Epoch 820 MSE = 0.524321\n",
            "Epoch 830 MSE = 0.52432096\n",
            "Epoch 840 MSE = 0.524321\n",
            "Epoch 850 MSE = 0.52432126\n",
            "Epoch 860 MSE = 0.524321\n",
            "Epoch 870 MSE = 0.524321\n",
            "Epoch 880 MSE = 0.5243209\n",
            "Epoch 890 MSE = 0.52432096\n",
            "Epoch 900 MSE = 0.52432084\n",
            "Epoch 910 MSE = 0.52432084\n",
            "Epoch 920 MSE = 0.5243208\n",
            "Epoch 930 MSE = 0.5243208\n",
            "Epoch 940 MSE = 0.5243207\n",
            "Epoch 950 MSE = 0.52432084\n",
            "Epoch 960 MSE = 0.5243208\n",
            "Epoch 970 MSE = 0.5243209\n",
            "Epoch 980 MSE = 0.52432096\n",
            "Epoch 990 MSE = 0.5243208\n",
            "Epoch 1000 MSE = 0.5243208\n",
            "Epoch 1010 MSE = 0.5243208\n",
            "Epoch 1020 MSE = 0.52432084\n",
            "Epoch 1030 MSE = 0.52432096\n",
            "Epoch 1040 MSE = 0.524321\n",
            "Epoch 1050 MSE = 0.52432096\n",
            "Epoch 1060 MSE = 0.524321\n",
            "Epoch 1070 MSE = 0.524321\n",
            "Epoch 1080 MSE = 0.52432096\n",
            "Epoch 1090 MSE = 0.52432096\n",
            "Epoch 1100 MSE = 0.52432096\n",
            "Epoch 1110 MSE = 0.52432096\n",
            "Epoch 1120 MSE = 0.52432096\n",
            "Epoch 1130 MSE = 0.52432096\n",
            "Epoch 1140 MSE = 0.52432096\n",
            "Epoch 1150 MSE = 0.52432084\n",
            "Epoch 1160 MSE = 0.52432084\n",
            "Epoch 1170 MSE = 0.52432084\n",
            "Epoch 1180 MSE = 0.52432084\n",
            "Epoch 1190 MSE = 0.52432084\n",
            "Epoch 1200 MSE = 0.52432084\n",
            "Epoch 1210 MSE = 0.52432084\n",
            "Epoch 1220 MSE = 0.5243209\n",
            "Epoch 1230 MSE = 0.5243209\n",
            "Epoch 1240 MSE = 0.52432084\n",
            "Epoch 1250 MSE = 0.52432096\n",
            "Epoch 1260 MSE = 0.5243209\n",
            "Epoch 1270 MSE = 0.52432096\n",
            "Epoch 1280 MSE = 0.524321\n",
            "Epoch 1290 MSE = 0.52432096\n",
            "Epoch 1300 MSE = 0.52432096\n",
            "Epoch 1310 MSE = 0.52432096\n",
            "Epoch 1320 MSE = 0.52432096\n",
            "Epoch 1330 MSE = 0.52432096\n",
            "Epoch 1340 MSE = 0.52432096\n",
            "Epoch 1350 MSE = 0.52432096\n",
            "Epoch 1360 MSE = 0.52432096\n",
            "Epoch 1370 MSE = 0.52432096\n",
            "Epoch 1380 MSE = 0.52432096\n",
            "Epoch 1390 MSE = 0.52432096\n",
            "Epoch 1400 MSE = 0.52432096\n",
            "Epoch 1410 MSE = 0.52432096\n",
            "Epoch 1420 MSE = 0.52432096\n",
            "Epoch 1430 MSE = 0.52432096\n",
            "Epoch 1440 MSE = 0.52432096\n",
            "Epoch 1450 MSE = 0.52432096\n",
            "Epoch 1460 MSE = 0.52432096\n",
            "Epoch 1470 MSE = 0.52432096\n",
            "Epoch 1480 MSE = 0.52432096\n",
            "Epoch 1490 MSE = 0.52432096\n",
            "Epoch 1500 MSE = 0.52432096\n",
            "Epoch 1510 MSE = 0.52432096\n",
            "Epoch 1520 MSE = 0.52432096\n",
            "Epoch 1530 MSE = 0.52432096\n",
            "Epoch 1540 MSE = 0.52432096\n",
            "Epoch 1550 MSE = 0.52432096\n",
            "Epoch 1560 MSE = 0.52432096\n",
            "Epoch 1570 MSE = 0.52432096\n",
            "Epoch 1580 MSE = 0.52432096\n",
            "Epoch 1590 MSE = 0.52432096\n",
            "Epoch 1600 MSE = 0.52432096\n",
            "Epoch 1610 MSE = 0.52432096\n",
            "Epoch 1620 MSE = 0.52432096\n",
            "Epoch 1630 MSE = 0.52432096\n",
            "Epoch 1640 MSE = 0.52432096\n",
            "Epoch 1650 MSE = 0.52432096\n",
            "Epoch 1660 MSE = 0.52432096\n",
            "Epoch 1670 MSE = 0.52432096\n",
            "Epoch 1680 MSE = 0.52432096\n",
            "Epoch 1690 MSE = 0.52432096\n",
            "Epoch 1700 MSE = 0.52432096\n",
            "Epoch 1710 MSE = 0.52432096\n",
            "Epoch 1720 MSE = 0.52432096\n",
            "Epoch 1730 MSE = 0.52432096\n",
            "Epoch 1740 MSE = 0.52432096\n",
            "Epoch 1750 MSE = 0.52432096\n",
            "Epoch 1760 MSE = 0.52432096\n",
            "Epoch 1770 MSE = 0.52432096\n",
            "Epoch 1780 MSE = 0.52432096\n",
            "Epoch 1790 MSE = 0.52432096\n",
            "Epoch 1800 MSE = 0.52432096\n",
            "Epoch 1810 MSE = 0.52432096\n",
            "Epoch 1820 MSE = 0.52432096\n",
            "Epoch 1830 MSE = 0.52432096\n",
            "Epoch 1840 MSE = 0.52432096\n",
            "Epoch 1850 MSE = 0.52432096\n",
            "Epoch 1860 MSE = 0.52432096\n",
            "Epoch 1870 MSE = 0.52432096\n",
            "Epoch 1880 MSE = 0.52432096\n",
            "Epoch 1890 MSE = 0.52432096\n",
            "Epoch 1900 MSE = 0.52432096\n",
            "Epoch 1910 MSE = 0.52432096\n",
            "Epoch 1920 MSE = 0.52432096\n",
            "Epoch 1930 MSE = 0.52432096\n",
            "Epoch 1940 MSE = 0.52432096\n",
            "Epoch 1950 MSE = 0.52432096\n",
            "Epoch 1960 MSE = 0.52432096\n",
            "Epoch 1970 MSE = 0.52432096\n",
            "Epoch 1980 MSE = 0.52432096\n",
            "Epoch 1990 MSE = 0.52432096\n",
            "Epoch 2000 MSE = 0.52432096\n",
            "Epoch 2010 MSE = 0.52432096\n",
            "Epoch 2020 MSE = 0.52432096\n",
            "Epoch 2030 MSE = 0.52432096\n",
            "Epoch 2040 MSE = 0.52432096\n",
            "Epoch 2050 MSE = 0.52432096\n",
            "Epoch 2060 MSE = 0.52432096\n",
            "Epoch 2070 MSE = 0.52432096\n",
            "Epoch 2080 MSE = 0.52432096\n",
            "Epoch 2090 MSE = 0.52432096\n",
            "Epoch 2100 MSE = 0.52432096\n",
            "Epoch 2110 MSE = 0.52432096\n",
            "Epoch 2120 MSE = 0.52432096\n",
            "Epoch 2130 MSE = 0.52432096\n",
            "Epoch 2140 MSE = 0.52432096\n",
            "Epoch 2150 MSE = 0.52432096\n",
            "Epoch 2160 MSE = 0.52432096\n",
            "Epoch 2170 MSE = 0.52432096\n",
            "Epoch 2180 MSE = 0.52432096\n",
            "Epoch 2190 MSE = 0.52432096\n",
            "Epoch 2200 MSE = 0.52432096\n",
            "Epoch 2210 MSE = 0.52432096\n",
            "Epoch 2220 MSE = 0.52432096\n",
            "Epoch 2230 MSE = 0.52432096\n",
            "Epoch 2240 MSE = 0.52432096\n",
            "Epoch 2250 MSE = 0.52432096\n",
            "Epoch 2260 MSE = 0.52432096\n",
            "Epoch 2270 MSE = 0.52432096\n",
            "Epoch 2280 MSE = 0.52432096\n",
            "Epoch 2290 MSE = 0.52432096\n",
            "Epoch 2300 MSE = 0.52432096\n",
            "Epoch 2310 MSE = 0.52432096\n",
            "Epoch 2320 MSE = 0.52432096\n",
            "Epoch 2330 MSE = 0.52432096\n",
            "Epoch 2340 MSE = 0.52432096\n",
            "Epoch 2350 MSE = 0.52432096\n",
            "Epoch 2360 MSE = 0.52432096\n",
            "Epoch 2370 MSE = 0.52432096\n",
            "Epoch 2380 MSE = 0.52432096\n",
            "Epoch 2390 MSE = 0.52432096\n",
            "Epoch 2400 MSE = 0.52432096\n",
            "Epoch 2410 MSE = 0.52432096\n",
            "Epoch 2420 MSE = 0.52432096\n",
            "Epoch 2430 MSE = 0.52432096\n",
            "Epoch 2440 MSE = 0.52432096\n",
            "Epoch 2450 MSE = 0.52432096\n",
            "Epoch 2460 MSE = 0.52432096\n",
            "Epoch 2470 MSE = 0.52432096\n",
            "Epoch 2480 MSE = 0.52432096\n",
            "Epoch 2490 MSE = 0.52432096\n",
            "Epoch 2500 MSE = 0.52432096\n",
            "Epoch 2510 MSE = 0.52432096\n",
            "Epoch 2520 MSE = 0.52432096\n",
            "Epoch 2530 MSE = 0.52432096\n",
            "Epoch 2540 MSE = 0.52432096\n",
            "Epoch 2550 MSE = 0.52432096\n",
            "Epoch 2560 MSE = 0.52432096\n",
            "Epoch 2570 MSE = 0.52432096\n",
            "Epoch 2580 MSE = 0.52432096\n",
            "Epoch 2590 MSE = 0.52432096\n",
            "Epoch 2600 MSE = 0.52432096\n",
            "Epoch 2610 MSE = 0.52432096\n",
            "Epoch 2620 MSE = 0.52432096\n",
            "Epoch 2630 MSE = 0.52432096\n",
            "Epoch 2640 MSE = 0.52432096\n",
            "Epoch 2650 MSE = 0.52432096\n",
            "Epoch 2660 MSE = 0.52432096\n",
            "Epoch 2670 MSE = 0.52432096\n",
            "Epoch 2680 MSE = 0.52432096\n",
            "Epoch 2690 MSE = 0.52432096\n",
            "Epoch 2700 MSE = 0.52432096\n",
            "Epoch 2710 MSE = 0.52432096\n",
            "Epoch 2720 MSE = 0.52432096\n",
            "Epoch 2730 MSE = 0.52432096\n",
            "Epoch 2740 MSE = 0.52432096\n",
            "Epoch 2750 MSE = 0.52432096\n",
            "Epoch 2760 MSE = 0.52432096\n",
            "Epoch 2770 MSE = 0.52432096\n",
            "Epoch 2780 MSE = 0.52432096\n",
            "Epoch 2790 MSE = 0.52432096\n",
            "Epoch 2800 MSE = 0.52432096\n",
            "Epoch 2810 MSE = 0.52432096\n",
            "Epoch 2820 MSE = 0.52432096\n",
            "Epoch 2830 MSE = 0.52432096\n",
            "Epoch 2840 MSE = 0.52432096\n",
            "Epoch 2850 MSE = 0.52432096\n",
            "Epoch 2860 MSE = 0.52432096\n",
            "Epoch 2870 MSE = 0.52432096\n",
            "Epoch 2880 MSE = 0.52432096\n",
            "Epoch 2890 MSE = 0.52432096\n",
            "Epoch 2900 MSE = 0.52432096\n",
            "Epoch 2910 MSE = 0.52432096\n",
            "Epoch 2920 MSE = 0.52432096\n",
            "Epoch 2930 MSE = 0.52432096\n",
            "Epoch 2940 MSE = 0.52432096\n",
            "Epoch 2950 MSE = 0.52432096\n",
            "Epoch 2960 MSE = 0.52432096\n",
            "Epoch 2970 MSE = 0.52432096\n",
            "Epoch 2980 MSE = 0.52432096\n",
            "Epoch 2990 MSE = 0.52432096\n",
            "Epoch 3000 MSE = 0.52432096\n",
            "Epoch 3010 MSE = 0.52432096\n",
            "Epoch 3020 MSE = 0.52432096\n",
            "Epoch 3030 MSE = 0.52432096\n",
            "Epoch 3040 MSE = 0.52432096\n",
            "Epoch 3050 MSE = 0.52432096\n",
            "Epoch 3060 MSE = 0.52432096\n",
            "Epoch 3070 MSE = 0.52432096\n",
            "Epoch 3080 MSE = 0.52432096\n",
            "Epoch 3090 MSE = 0.52432096\n",
            "Epoch 3100 MSE = 0.52432096\n",
            "Epoch 3110 MSE = 0.52432096\n",
            "Epoch 3120 MSE = 0.52432096\n",
            "Epoch 3130 MSE = 0.52432096\n",
            "Epoch 3140 MSE = 0.52432096\n",
            "Epoch 3150 MSE = 0.52432096\n",
            "Epoch 3160 MSE = 0.52432096\n",
            "Epoch 3170 MSE = 0.52432096\n",
            "Epoch 3180 MSE = 0.52432096\n",
            "Epoch 3190 MSE = 0.52432096\n",
            "Epoch 3200 MSE = 0.52432096\n",
            "Epoch 3210 MSE = 0.52432096\n",
            "Epoch 3220 MSE = 0.52432096\n",
            "Epoch 3230 MSE = 0.52432096\n",
            "Epoch 3240 MSE = 0.52432096\n",
            "Epoch 3250 MSE = 0.52432096\n",
            "Epoch 3260 MSE = 0.52432096\n",
            "Epoch 3270 MSE = 0.52432096\n",
            "Epoch 3280 MSE = 0.52432096\n",
            "Epoch 3290 MSE = 0.52432096\n",
            "Epoch 3300 MSE = 0.52432096\n",
            "Epoch 3310 MSE = 0.52432096\n",
            "Epoch 3320 MSE = 0.52432096\n",
            "Epoch 3330 MSE = 0.52432096\n",
            "Epoch 3340 MSE = 0.52432096\n",
            "Epoch 3350 MSE = 0.52432096\n",
            "Epoch 3360 MSE = 0.52432096\n",
            "Epoch 3370 MSE = 0.52432096\n",
            "Epoch 3380 MSE = 0.52432096\n",
            "Epoch 3390 MSE = 0.52432096\n",
            "Epoch 3400 MSE = 0.52432096\n",
            "Epoch 3410 MSE = 0.52432096\n",
            "Epoch 3420 MSE = 0.52432096\n",
            "Epoch 3430 MSE = 0.52432096\n",
            "Epoch 3440 MSE = 0.52432096\n",
            "Epoch 3450 MSE = 0.52432096\n",
            "Epoch 3460 MSE = 0.52432096\n",
            "Epoch 3470 MSE = 0.52432096\n",
            "Epoch 3480 MSE = 0.52432096\n",
            "Epoch 3490 MSE = 0.52432096\n",
            "Epoch 3500 MSE = 0.52432096\n",
            "Epoch 3510 MSE = 0.52432096\n",
            "Epoch 3520 MSE = 0.52432096\n",
            "Epoch 3530 MSE = 0.52432096\n",
            "Epoch 3540 MSE = 0.52432096\n",
            "Epoch 3550 MSE = 0.52432096\n",
            "Epoch 3560 MSE = 0.52432096\n",
            "Epoch 3570 MSE = 0.52432096\n",
            "Epoch 3580 MSE = 0.52432096\n",
            "Epoch 3590 MSE = 0.52432096\n",
            "Epoch 3600 MSE = 0.52432096\n",
            "Epoch 3610 MSE = 0.52432096\n",
            "Epoch 3620 MSE = 0.52432096\n",
            "Epoch 3630 MSE = 0.52432096\n",
            "Epoch 3640 MSE = 0.52432096\n",
            "Epoch 3650 MSE = 0.52432096\n",
            "Epoch 3660 MSE = 0.52432096\n",
            "Epoch 3670 MSE = 0.52432096\n",
            "Epoch 3680 MSE = 0.52432096\n",
            "Epoch 3690 MSE = 0.52432096\n",
            "Epoch 3700 MSE = 0.52432096\n",
            "Epoch 3710 MSE = 0.52432096\n",
            "Epoch 3720 MSE = 0.52432096\n",
            "Epoch 3730 MSE = 0.52432096\n",
            "Epoch 3740 MSE = 0.52432096\n",
            "Epoch 3750 MSE = 0.52432096\n",
            "Epoch 3760 MSE = 0.52432096\n",
            "Epoch 3770 MSE = 0.52432096\n",
            "Epoch 3780 MSE = 0.52432096\n",
            "Epoch 3790 MSE = 0.52432096\n",
            "Epoch 3800 MSE = 0.52432096\n",
            "Epoch 3810 MSE = 0.52432096\n",
            "Epoch 3820 MSE = 0.52432096\n",
            "Epoch 3830 MSE = 0.52432096\n",
            "Epoch 3840 MSE = 0.52432096\n",
            "Epoch 3850 MSE = 0.52432096\n",
            "Epoch 3860 MSE = 0.52432096\n",
            "Epoch 3870 MSE = 0.52432096\n",
            "Epoch 3880 MSE = 0.52432096\n",
            "Epoch 3890 MSE = 0.52432096\n",
            "Epoch 3900 MSE = 0.52432096\n",
            "Epoch 3910 MSE = 0.52432096\n",
            "Epoch 3920 MSE = 0.52432096\n",
            "Epoch 3930 MSE = 0.52432096\n",
            "Epoch 3940 MSE = 0.52432096\n",
            "Epoch 3950 MSE = 0.52432096\n",
            "Epoch 3960 MSE = 0.52432096\n",
            "Epoch 3970 MSE = 0.52432096\n",
            "Epoch 3980 MSE = 0.52432096\n",
            "Epoch 3990 MSE = 0.52432096\n",
            "Epoch 4000 MSE = 0.52432096\n",
            "Epoch 4010 MSE = 0.52432096\n",
            "Epoch 4020 MSE = 0.52432096\n",
            "Epoch 4030 MSE = 0.52432096\n",
            "Epoch 4040 MSE = 0.52432096\n",
            "Epoch 4050 MSE = 0.52432096\n",
            "Epoch 4060 MSE = 0.52432096\n",
            "Epoch 4070 MSE = 0.52432096\n",
            "Epoch 4080 MSE = 0.52432096\n",
            "Epoch 4090 MSE = 0.52432096\n",
            "Epoch 4100 MSE = 0.52432096\n",
            "Epoch 4110 MSE = 0.52432096\n",
            "Epoch 4120 MSE = 0.52432096\n",
            "Epoch 4130 MSE = 0.52432096\n",
            "Epoch 4140 MSE = 0.52432096\n",
            "Epoch 4150 MSE = 0.52432096\n",
            "Epoch 4160 MSE = 0.52432096\n",
            "Epoch 4170 MSE = 0.52432096\n",
            "Epoch 4180 MSE = 0.52432096\n",
            "Epoch 4190 MSE = 0.52432096\n",
            "Epoch 4200 MSE = 0.52432096\n",
            "Epoch 4210 MSE = 0.52432096\n",
            "Epoch 4220 MSE = 0.52432096\n",
            "Epoch 4230 MSE = 0.52432096\n",
            "Epoch 4240 MSE = 0.52432096\n",
            "Epoch 4250 MSE = 0.52432096\n",
            "Epoch 4260 MSE = 0.52432096\n",
            "Epoch 4270 MSE = 0.52432096\n",
            "Epoch 4280 MSE = 0.52432096\n",
            "Epoch 4290 MSE = 0.52432096\n",
            "Epoch 4300 MSE = 0.52432096\n",
            "Epoch 4310 MSE = 0.52432096\n",
            "Epoch 4320 MSE = 0.52432096\n",
            "Epoch 4330 MSE = 0.52432096\n",
            "Epoch 4340 MSE = 0.52432096\n",
            "Epoch 4350 MSE = 0.52432096\n",
            "Epoch 4360 MSE = 0.52432096\n",
            "Epoch 4370 MSE = 0.52432096\n",
            "Epoch 4380 MSE = 0.52432096\n",
            "Epoch 4390 MSE = 0.52432096\n",
            "Epoch 4400 MSE = 0.52432096\n",
            "Epoch 4410 MSE = 0.52432096\n",
            "Epoch 4420 MSE = 0.52432096\n",
            "Epoch 4430 MSE = 0.52432096\n",
            "Epoch 4440 MSE = 0.52432096\n",
            "Epoch 4450 MSE = 0.52432096\n",
            "Epoch 4460 MSE = 0.52432096\n",
            "Epoch 4470 MSE = 0.52432096\n",
            "Epoch 4480 MSE = 0.52432096\n",
            "Epoch 4490 MSE = 0.52432096\n",
            "Epoch 4500 MSE = 0.52432096\n",
            "Epoch 4510 MSE = 0.52432096\n",
            "Epoch 4520 MSE = 0.52432096\n",
            "Epoch 4530 MSE = 0.52432096\n",
            "Epoch 4540 MSE = 0.52432096\n",
            "Epoch 4550 MSE = 0.52432096\n",
            "Epoch 4560 MSE = 0.52432096\n",
            "Epoch 4570 MSE = 0.52432096\n",
            "Epoch 4580 MSE = 0.52432096\n",
            "Epoch 4590 MSE = 0.52432096\n",
            "Epoch 4600 MSE = 0.52432096\n",
            "Epoch 4610 MSE = 0.52432096\n",
            "Epoch 4620 MSE = 0.52432096\n",
            "Epoch 4630 MSE = 0.52432096\n",
            "Epoch 4640 MSE = 0.52432096\n",
            "Epoch 4650 MSE = 0.52432096\n",
            "Epoch 4660 MSE = 0.52432096\n",
            "Epoch 4670 MSE = 0.52432096\n",
            "Epoch 4680 MSE = 0.52432096\n",
            "Epoch 4690 MSE = 0.52432096\n",
            "Epoch 4700 MSE = 0.52432096\n",
            "Epoch 4710 MSE = 0.52432096\n",
            "Epoch 4720 MSE = 0.52432096\n",
            "Epoch 4730 MSE = 0.52432096\n",
            "Epoch 4740 MSE = 0.52432096\n",
            "Epoch 4750 MSE = 0.52432096\n",
            "Epoch 4760 MSE = 0.52432096\n",
            "Epoch 4770 MSE = 0.52432096\n",
            "Epoch 4780 MSE = 0.52432096\n",
            "Epoch 4790 MSE = 0.52432096\n",
            "Epoch 4800 MSE = 0.52432096\n",
            "Epoch 4810 MSE = 0.52432096\n",
            "Epoch 4820 MSE = 0.52432096\n",
            "Epoch 4830 MSE = 0.52432096\n",
            "Epoch 4840 MSE = 0.52432096\n",
            "Epoch 4850 MSE = 0.52432096\n",
            "Epoch 4860 MSE = 0.52432096\n",
            "Epoch 4870 MSE = 0.52432096\n",
            "Epoch 4880 MSE = 0.52432096\n",
            "Epoch 4890 MSE = 0.52432096\n",
            "Epoch 4900 MSE = 0.52432096\n",
            "Epoch 4910 MSE = 0.52432096\n",
            "Epoch 4920 MSE = 0.52432096\n",
            "Epoch 4930 MSE = 0.52432096\n",
            "Epoch 4940 MSE = 0.52432096\n",
            "Epoch 4950 MSE = 0.52432096\n",
            "Epoch 4960 MSE = 0.52432096\n",
            "Epoch 4970 MSE = 0.52432096\n",
            "Epoch 4980 MSE = 0.52432096\n",
            "Epoch 4990 MSE = 0.52432096\n",
            "Epoch 5000 MSE = 0.52432096\n",
            "Epoch 5010 MSE = 0.52432096\n",
            "Epoch 5020 MSE = 0.52432096\n",
            "Epoch 5030 MSE = 0.52432096\n",
            "Epoch 5040 MSE = 0.52432096\n",
            "Epoch 5050 MSE = 0.52432096\n",
            "Epoch 5060 MSE = 0.52432096\n",
            "Epoch 5070 MSE = 0.52432096\n",
            "Epoch 5080 MSE = 0.52432096\n",
            "Epoch 5090 MSE = 0.52432096\n",
            "Epoch 5100 MSE = 0.52432096\n",
            "Epoch 5110 MSE = 0.52432096\n",
            "Epoch 5120 MSE = 0.52432096\n",
            "Epoch 5130 MSE = 0.52432096\n",
            "Epoch 5140 MSE = 0.52432096\n",
            "Epoch 5150 MSE = 0.52432096\n",
            "Epoch 5160 MSE = 0.52432096\n",
            "Epoch 5170 MSE = 0.52432096\n",
            "Epoch 5180 MSE = 0.52432096\n",
            "Epoch 5190 MSE = 0.52432096\n",
            "Epoch 5200 MSE = 0.52432096\n",
            "Epoch 5210 MSE = 0.52432096\n",
            "Epoch 5220 MSE = 0.52432096\n",
            "Epoch 5230 MSE = 0.52432096\n",
            "Epoch 5240 MSE = 0.52432096\n",
            "Epoch 5250 MSE = 0.52432096\n",
            "Epoch 5260 MSE = 0.52432096\n",
            "Epoch 5270 MSE = 0.52432096\n",
            "Epoch 5280 MSE = 0.52432096\n",
            "Epoch 5290 MSE = 0.52432096\n",
            "Epoch 5300 MSE = 0.52432096\n",
            "Epoch 5310 MSE = 0.52432096\n",
            "Epoch 5320 MSE = 0.52432096\n",
            "Epoch 5330 MSE = 0.52432096\n",
            "Epoch 5340 MSE = 0.52432096\n",
            "Epoch 5350 MSE = 0.52432096\n",
            "Epoch 5360 MSE = 0.52432096\n",
            "Epoch 5370 MSE = 0.52432096\n",
            "Epoch 5380 MSE = 0.52432096\n",
            "Epoch 5390 MSE = 0.52432096\n",
            "Epoch 5400 MSE = 0.52432096\n",
            "Epoch 5410 MSE = 0.52432096\n",
            "Epoch 5420 MSE = 0.52432096\n",
            "Epoch 5430 MSE = 0.52432096\n",
            "Epoch 5440 MSE = 0.52432096\n",
            "Epoch 5450 MSE = 0.52432096\n",
            "Epoch 5460 MSE = 0.52432096\n",
            "Epoch 5470 MSE = 0.52432096\n",
            "Epoch 5480 MSE = 0.52432096\n",
            "Epoch 5490 MSE = 0.52432096\n",
            "Epoch 5500 MSE = 0.52432096\n",
            "Epoch 5510 MSE = 0.52432096\n",
            "Epoch 5520 MSE = 0.52432096\n",
            "Epoch 5530 MSE = 0.52432096\n",
            "Epoch 5540 MSE = 0.52432096\n",
            "Epoch 5550 MSE = 0.52432096\n",
            "Epoch 5560 MSE = 0.52432096\n",
            "Epoch 5570 MSE = 0.52432096\n",
            "Epoch 5580 MSE = 0.52432096\n",
            "Epoch 5590 MSE = 0.52432096\n",
            "Epoch 5600 MSE = 0.52432096\n",
            "Epoch 5610 MSE = 0.52432096\n",
            "Epoch 5620 MSE = 0.52432096\n",
            "Epoch 5630 MSE = 0.52432096\n",
            "Epoch 5640 MSE = 0.52432096\n",
            "Epoch 5650 MSE = 0.52432096\n",
            "Epoch 5660 MSE = 0.52432096\n",
            "Epoch 5670 MSE = 0.52432096\n",
            "Epoch 5680 MSE = 0.52432096\n",
            "Epoch 5690 MSE = 0.52432096\n",
            "Epoch 5700 MSE = 0.52432096\n",
            "Epoch 5710 MSE = 0.52432096\n",
            "Epoch 5720 MSE = 0.52432096\n",
            "Epoch 5730 MSE = 0.52432096\n",
            "Epoch 5740 MSE = 0.52432096\n",
            "Epoch 5750 MSE = 0.52432096\n",
            "Epoch 5760 MSE = 0.52432096\n",
            "Epoch 5770 MSE = 0.52432096\n",
            "Epoch 5780 MSE = 0.52432096\n",
            "Epoch 5790 MSE = 0.52432096\n",
            "Epoch 5800 MSE = 0.52432096\n",
            "Epoch 5810 MSE = 0.52432096\n",
            "Epoch 5820 MSE = 0.52432096\n",
            "Epoch 5830 MSE = 0.52432096\n",
            "Epoch 5840 MSE = 0.52432096\n",
            "Epoch 5850 MSE = 0.52432096\n",
            "Epoch 5860 MSE = 0.52432096\n",
            "Epoch 5870 MSE = 0.52432096\n",
            "Epoch 5880 MSE = 0.52432096\n",
            "Epoch 5890 MSE = 0.52432096\n",
            "Epoch 5900 MSE = 0.52432096\n",
            "Epoch 5910 MSE = 0.52432096\n",
            "Epoch 5920 MSE = 0.52432096\n",
            "Epoch 5930 MSE = 0.52432096\n",
            "Epoch 5940 MSE = 0.52432096\n",
            "Epoch 5950 MSE = 0.52432096\n",
            "Epoch 5960 MSE = 0.52432096\n",
            "Epoch 5970 MSE = 0.52432096\n",
            "Epoch 5980 MSE = 0.52432096\n",
            "Epoch 5990 MSE = 0.52432096\n",
            "Epoch 6000 MSE = 0.52432096\n",
            "Epoch 6010 MSE = 0.52432096\n",
            "Epoch 6020 MSE = 0.52432096\n",
            "Epoch 6030 MSE = 0.52432096\n",
            "Epoch 6040 MSE = 0.52432096\n",
            "Epoch 6050 MSE = 0.52432096\n",
            "Epoch 6060 MSE = 0.52432096\n",
            "Epoch 6070 MSE = 0.52432096\n",
            "Epoch 6080 MSE = 0.52432096\n",
            "Epoch 6090 MSE = 0.52432096\n",
            "Epoch 6100 MSE = 0.52432096\n",
            "Epoch 6110 MSE = 0.52432096\n",
            "Epoch 6120 MSE = 0.52432096\n",
            "Epoch 6130 MSE = 0.52432096\n",
            "Epoch 6140 MSE = 0.52432096\n",
            "Epoch 6150 MSE = 0.52432096\n",
            "Epoch 6160 MSE = 0.52432096\n",
            "Epoch 6170 MSE = 0.52432096\n",
            "Epoch 6180 MSE = 0.52432096\n",
            "Epoch 6190 MSE = 0.52432096\n",
            "Epoch 6200 MSE = 0.52432096\n",
            "Epoch 6210 MSE = 0.52432096\n",
            "Epoch 6220 MSE = 0.52432096\n",
            "Epoch 6230 MSE = 0.52432096\n",
            "Epoch 6240 MSE = 0.52432096\n",
            "Epoch 6250 MSE = 0.52432096\n",
            "Epoch 6260 MSE = 0.52432096\n",
            "Epoch 6270 MSE = 0.52432096\n",
            "Epoch 6280 MSE = 0.52432096\n",
            "Epoch 6290 MSE = 0.52432096\n",
            "Epoch 6300 MSE = 0.52432096\n",
            "Epoch 6310 MSE = 0.52432096\n",
            "Epoch 6320 MSE = 0.52432096\n",
            "Epoch 6330 MSE = 0.52432096\n",
            "Epoch 6340 MSE = 0.52432096\n",
            "Epoch 6350 MSE = 0.52432096\n",
            "Epoch 6360 MSE = 0.52432096\n",
            "Epoch 6370 MSE = 0.52432096\n",
            "Epoch 6380 MSE = 0.52432096\n",
            "Epoch 6390 MSE = 0.52432096\n",
            "Epoch 6400 MSE = 0.52432096\n",
            "Epoch 6410 MSE = 0.52432096\n",
            "Epoch 6420 MSE = 0.52432096\n",
            "Epoch 6430 MSE = 0.52432096\n",
            "Epoch 6440 MSE = 0.52432096\n",
            "Epoch 6450 MSE = 0.52432096\n",
            "Epoch 6460 MSE = 0.52432096\n",
            "Epoch 6470 MSE = 0.52432096\n",
            "Epoch 6480 MSE = 0.52432096\n",
            "Epoch 6490 MSE = 0.52432096\n",
            "Epoch 6500 MSE = 0.52432096\n",
            "Epoch 6510 MSE = 0.52432096\n",
            "Epoch 6520 MSE = 0.52432096\n",
            "Epoch 6530 MSE = 0.52432096\n",
            "Epoch 6540 MSE = 0.52432096\n",
            "Epoch 6550 MSE = 0.52432096\n",
            "Epoch 6560 MSE = 0.52432096\n",
            "Epoch 6570 MSE = 0.52432096\n",
            "Epoch 6580 MSE = 0.52432096\n",
            "Epoch 6590 MSE = 0.52432096\n",
            "Epoch 6600 MSE = 0.52432096\n",
            "Epoch 6610 MSE = 0.52432096\n",
            "Epoch 6620 MSE = 0.52432096\n",
            "Epoch 6630 MSE = 0.52432096\n",
            "Epoch 6640 MSE = 0.52432096\n",
            "Epoch 6650 MSE = 0.52432096\n",
            "Epoch 6660 MSE = 0.52432096\n",
            "Epoch 6670 MSE = 0.52432096\n",
            "Epoch 6680 MSE = 0.52432096\n",
            "Epoch 6690 MSE = 0.52432096\n",
            "Epoch 6700 MSE = 0.52432096\n",
            "Epoch 6710 MSE = 0.52432096\n",
            "Epoch 6720 MSE = 0.52432096\n",
            "Epoch 6730 MSE = 0.52432096\n",
            "Epoch 6740 MSE = 0.52432096\n",
            "Epoch 6750 MSE = 0.52432096\n",
            "Epoch 6760 MSE = 0.52432096\n",
            "Epoch 6770 MSE = 0.52432096\n",
            "Epoch 6780 MSE = 0.52432096\n",
            "Epoch 6790 MSE = 0.52432096\n",
            "Epoch 6800 MSE = 0.52432096\n",
            "Epoch 6810 MSE = 0.52432096\n",
            "Epoch 6820 MSE = 0.52432096\n",
            "Epoch 6830 MSE = 0.52432096\n",
            "Epoch 6840 MSE = 0.52432096\n",
            "Epoch 6850 MSE = 0.52432096\n",
            "Epoch 6860 MSE = 0.52432096\n",
            "Epoch 6870 MSE = 0.52432096\n",
            "Epoch 6880 MSE = 0.52432096\n",
            "Epoch 6890 MSE = 0.52432096\n",
            "Epoch 6900 MSE = 0.52432096\n",
            "Epoch 6910 MSE = 0.52432096\n",
            "Epoch 6920 MSE = 0.52432096\n",
            "Epoch 6930 MSE = 0.52432096\n",
            "Epoch 6940 MSE = 0.52432096\n",
            "Epoch 6950 MSE = 0.52432096\n",
            "Epoch 6960 MSE = 0.52432096\n",
            "Epoch 6970 MSE = 0.52432096\n",
            "Epoch 6980 MSE = 0.52432096\n",
            "Epoch 6990 MSE = 0.52432096\n",
            "Epoch 7000 MSE = 0.52432096\n",
            "Epoch 7010 MSE = 0.52432096\n",
            "Epoch 7020 MSE = 0.52432096\n",
            "Epoch 7030 MSE = 0.52432096\n",
            "Epoch 7040 MSE = 0.52432096\n",
            "Epoch 7050 MSE = 0.52432096\n",
            "Epoch 7060 MSE = 0.52432096\n",
            "Epoch 7070 MSE = 0.52432096\n",
            "Epoch 7080 MSE = 0.52432096\n",
            "Epoch 7090 MSE = 0.52432096\n",
            "Epoch 7100 MSE = 0.52432096\n",
            "Epoch 7110 MSE = 0.52432096\n",
            "Epoch 7120 MSE = 0.52432096\n",
            "Epoch 7130 MSE = 0.52432096\n",
            "Epoch 7140 MSE = 0.52432096\n",
            "Epoch 7150 MSE = 0.52432096\n",
            "Epoch 7160 MSE = 0.52432096\n",
            "Epoch 7170 MSE = 0.52432096\n",
            "Epoch 7180 MSE = 0.52432096\n",
            "Epoch 7190 MSE = 0.52432096\n",
            "Epoch 7200 MSE = 0.52432096\n",
            "Epoch 7210 MSE = 0.52432096\n",
            "Epoch 7220 MSE = 0.52432096\n",
            "Epoch 7230 MSE = 0.52432096\n",
            "Epoch 7240 MSE = 0.52432096\n",
            "Epoch 7250 MSE = 0.52432096\n",
            "Epoch 7260 MSE = 0.52432096\n",
            "Epoch 7270 MSE = 0.52432096\n",
            "Epoch 7280 MSE = 0.52432096\n",
            "Epoch 7290 MSE = 0.52432096\n",
            "Epoch 7300 MSE = 0.52432096\n",
            "Epoch 7310 MSE = 0.52432096\n",
            "Epoch 7320 MSE = 0.52432096\n",
            "Epoch 7330 MSE = 0.52432096\n",
            "Epoch 7340 MSE = 0.52432096\n",
            "Epoch 7350 MSE = 0.52432096\n",
            "Epoch 7360 MSE = 0.52432096\n",
            "Epoch 7370 MSE = 0.52432096\n",
            "Epoch 7380 MSE = 0.52432096\n",
            "Epoch 7390 MSE = 0.52432096\n",
            "Epoch 7400 MSE = 0.52432096\n",
            "Epoch 7410 MSE = 0.52432096\n",
            "Epoch 7420 MSE = 0.52432096\n",
            "Epoch 7430 MSE = 0.52432096\n",
            "Epoch 7440 MSE = 0.52432096\n",
            "Epoch 7450 MSE = 0.52432096\n",
            "Epoch 7460 MSE = 0.52432096\n",
            "Epoch 7470 MSE = 0.52432096\n",
            "Epoch 7480 MSE = 0.52432096\n",
            "Epoch 7490 MSE = 0.52432096\n",
            "Epoch 7500 MSE = 0.52432096\n",
            "Epoch 7510 MSE = 0.52432096\n",
            "Epoch 7520 MSE = 0.52432096\n",
            "Epoch 7530 MSE = 0.52432096\n",
            "Epoch 7540 MSE = 0.52432096\n",
            "Epoch 7550 MSE = 0.52432096\n",
            "Epoch 7560 MSE = 0.52432096\n",
            "Epoch 7570 MSE = 0.52432096\n",
            "Epoch 7580 MSE = 0.52432096\n",
            "Epoch 7590 MSE = 0.52432096\n",
            "Epoch 7600 MSE = 0.52432096\n",
            "Epoch 7610 MSE = 0.52432096\n",
            "Epoch 7620 MSE = 0.52432096\n",
            "Epoch 7630 MSE = 0.52432096\n",
            "Epoch 7640 MSE = 0.52432096\n",
            "Epoch 7650 MSE = 0.52432096\n",
            "Epoch 7660 MSE = 0.52432096\n",
            "Epoch 7670 MSE = 0.52432096\n",
            "Epoch 7680 MSE = 0.52432096\n",
            "Epoch 7690 MSE = 0.52432096\n",
            "Epoch 7700 MSE = 0.52432096\n",
            "Epoch 7710 MSE = 0.52432096\n",
            "Epoch 7720 MSE = 0.52432096\n",
            "Epoch 7730 MSE = 0.52432096\n",
            "Epoch 7740 MSE = 0.52432096\n",
            "Epoch 7750 MSE = 0.52432096\n",
            "Epoch 7760 MSE = 0.52432096\n",
            "Epoch 7770 MSE = 0.52432096\n",
            "Epoch 7780 MSE = 0.52432096\n",
            "Epoch 7790 MSE = 0.52432096\n",
            "Epoch 7800 MSE = 0.52432096\n",
            "Epoch 7810 MSE = 0.52432096\n",
            "Epoch 7820 MSE = 0.52432096\n",
            "Epoch 7830 MSE = 0.52432096\n",
            "Epoch 7840 MSE = 0.52432096\n",
            "Epoch 7850 MSE = 0.52432096\n",
            "Epoch 7860 MSE = 0.52432096\n",
            "Epoch 7870 MSE = 0.52432096\n",
            "Epoch 7880 MSE = 0.52432096\n",
            "Epoch 7890 MSE = 0.52432096\n",
            "Epoch 7900 MSE = 0.52432096\n",
            "Epoch 7910 MSE = 0.52432096\n",
            "Epoch 7920 MSE = 0.52432096\n",
            "Epoch 7930 MSE = 0.52432096\n",
            "Epoch 7940 MSE = 0.52432096\n",
            "Epoch 7950 MSE = 0.52432096\n",
            "Epoch 7960 MSE = 0.52432096\n",
            "Epoch 7970 MSE = 0.52432096\n",
            "Epoch 7980 MSE = 0.52432096\n",
            "Epoch 7990 MSE = 0.52432096\n",
            "Epoch 8000 MSE = 0.52432096\n",
            "Epoch 8010 MSE = 0.52432096\n",
            "Epoch 8020 MSE = 0.52432096\n",
            "Epoch 8030 MSE = 0.52432096\n",
            "Epoch 8040 MSE = 0.52432096\n",
            "Epoch 8050 MSE = 0.52432096\n",
            "Epoch 8060 MSE = 0.52432096\n",
            "Epoch 8070 MSE = 0.52432096\n",
            "Epoch 8080 MSE = 0.52432096\n",
            "Epoch 8090 MSE = 0.52432096\n",
            "Epoch 8100 MSE = 0.52432096\n",
            "Epoch 8110 MSE = 0.52432096\n",
            "Epoch 8120 MSE = 0.52432096\n",
            "Epoch 8130 MSE = 0.52432096\n",
            "Epoch 8140 MSE = 0.52432096\n",
            "Epoch 8150 MSE = 0.52432096\n",
            "Epoch 8160 MSE = 0.52432096\n",
            "Epoch 8170 MSE = 0.52432096\n",
            "Epoch 8180 MSE = 0.52432096\n",
            "Epoch 8190 MSE = 0.52432096\n",
            "Epoch 8200 MSE = 0.52432096\n",
            "Epoch 8210 MSE = 0.52432096\n",
            "Epoch 8220 MSE = 0.52432096\n",
            "Epoch 8230 MSE = 0.52432096\n",
            "Epoch 8240 MSE = 0.52432096\n",
            "Epoch 8250 MSE = 0.52432096\n",
            "Epoch 8260 MSE = 0.52432096\n",
            "Epoch 8270 MSE = 0.52432096\n",
            "Epoch 8280 MSE = 0.52432096\n",
            "Epoch 8290 MSE = 0.52432096\n",
            "Epoch 8300 MSE = 0.52432096\n",
            "Epoch 8310 MSE = 0.52432096\n",
            "Epoch 8320 MSE = 0.52432096\n",
            "Epoch 8330 MSE = 0.52432096\n",
            "Epoch 8340 MSE = 0.52432096\n",
            "Epoch 8350 MSE = 0.52432096\n",
            "Epoch 8360 MSE = 0.52432096\n",
            "Epoch 8370 MSE = 0.52432096\n",
            "Epoch 8380 MSE = 0.52432096\n",
            "Epoch 8390 MSE = 0.52432096\n",
            "Epoch 8400 MSE = 0.52432096\n",
            "Epoch 8410 MSE = 0.52432096\n",
            "Epoch 8420 MSE = 0.52432096\n",
            "Epoch 8430 MSE = 0.52432096\n",
            "Epoch 8440 MSE = 0.52432096\n",
            "Epoch 8450 MSE = 0.52432096\n",
            "Epoch 8460 MSE = 0.52432096\n",
            "Epoch 8470 MSE = 0.52432096\n",
            "Epoch 8480 MSE = 0.52432096\n",
            "Epoch 8490 MSE = 0.52432096\n",
            "Epoch 8500 MSE = 0.52432096\n",
            "Epoch 8510 MSE = 0.52432096\n",
            "Epoch 8520 MSE = 0.52432096\n",
            "Epoch 8530 MSE = 0.52432096\n",
            "Epoch 8540 MSE = 0.52432096\n",
            "Epoch 8550 MSE = 0.52432096\n",
            "Epoch 8560 MSE = 0.52432096\n",
            "Epoch 8570 MSE = 0.52432096\n",
            "Epoch 8580 MSE = 0.52432096\n",
            "Epoch 8590 MSE = 0.52432096\n",
            "Epoch 8600 MSE = 0.52432096\n",
            "Epoch 8610 MSE = 0.52432096\n",
            "Epoch 8620 MSE = 0.52432096\n",
            "Epoch 8630 MSE = 0.52432096\n",
            "Epoch 8640 MSE = 0.52432096\n",
            "Epoch 8650 MSE = 0.52432096\n",
            "Epoch 8660 MSE = 0.52432096\n",
            "Epoch 8670 MSE = 0.52432096\n",
            "Epoch 8680 MSE = 0.52432096\n",
            "Epoch 8690 MSE = 0.52432096\n",
            "Epoch 8700 MSE = 0.52432096\n",
            "Epoch 8710 MSE = 0.52432096\n",
            "Epoch 8720 MSE = 0.52432096\n",
            "Epoch 8730 MSE = 0.52432096\n",
            "Epoch 8740 MSE = 0.52432096\n",
            "Epoch 8750 MSE = 0.52432096\n",
            "Epoch 8760 MSE = 0.52432096\n",
            "Epoch 8770 MSE = 0.52432096\n",
            "Epoch 8780 MSE = 0.52432096\n",
            "Epoch 8790 MSE = 0.52432096\n",
            "Epoch 8800 MSE = 0.52432096\n",
            "Epoch 8810 MSE = 0.52432096\n",
            "Epoch 8820 MSE = 0.52432096\n",
            "Epoch 8830 MSE = 0.52432096\n",
            "Epoch 8840 MSE = 0.52432096\n",
            "Epoch 8850 MSE = 0.52432096\n",
            "Epoch 8860 MSE = 0.52432096\n",
            "Epoch 8870 MSE = 0.52432096\n",
            "Epoch 8880 MSE = 0.52432096\n",
            "Epoch 8890 MSE = 0.52432096\n",
            "Epoch 8900 MSE = 0.52432096\n",
            "Epoch 8910 MSE = 0.52432096\n",
            "Epoch 8920 MSE = 0.52432096\n",
            "Epoch 8930 MSE = 0.52432096\n",
            "Epoch 8940 MSE = 0.52432096\n",
            "Epoch 8950 MSE = 0.52432096\n",
            "Epoch 8960 MSE = 0.52432096\n",
            "Epoch 8970 MSE = 0.52432096\n",
            "Epoch 8980 MSE = 0.52432096\n",
            "Epoch 8990 MSE = 0.52432096\n",
            "Epoch 9000 MSE = 0.52432096\n",
            "Epoch 9010 MSE = 0.52432096\n",
            "Epoch 9020 MSE = 0.52432096\n",
            "Epoch 9030 MSE = 0.52432096\n",
            "Epoch 9040 MSE = 0.52432096\n",
            "Epoch 9050 MSE = 0.52432096\n",
            "Epoch 9060 MSE = 0.52432096\n",
            "Epoch 9070 MSE = 0.52432096\n",
            "Epoch 9080 MSE = 0.52432096\n",
            "Epoch 9090 MSE = 0.52432096\n",
            "Epoch 9100 MSE = 0.52432096\n",
            "Epoch 9110 MSE = 0.52432096\n",
            "Epoch 9120 MSE = 0.52432096\n",
            "Epoch 9130 MSE = 0.52432096\n",
            "Epoch 9140 MSE = 0.52432096\n",
            "Epoch 9150 MSE = 0.52432096\n",
            "Epoch 9160 MSE = 0.52432096\n",
            "Epoch 9170 MSE = 0.52432096\n",
            "Epoch 9180 MSE = 0.52432096\n",
            "Epoch 9190 MSE = 0.52432096\n",
            "Epoch 9200 MSE = 0.52432096\n",
            "Epoch 9210 MSE = 0.52432096\n",
            "Epoch 9220 MSE = 0.52432096\n",
            "Epoch 9230 MSE = 0.52432096\n",
            "Epoch 9240 MSE = 0.52432096\n",
            "Epoch 9250 MSE = 0.52432096\n",
            "Epoch 9260 MSE = 0.52432096\n",
            "Epoch 9270 MSE = 0.52432096\n",
            "Epoch 9280 MSE = 0.52432096\n",
            "Epoch 9290 MSE = 0.52432096\n",
            "Epoch 9300 MSE = 0.52432096\n",
            "Epoch 9310 MSE = 0.52432096\n",
            "Epoch 9320 MSE = 0.52432096\n",
            "Epoch 9330 MSE = 0.52432096\n",
            "Epoch 9340 MSE = 0.52432096\n",
            "Epoch 9350 MSE = 0.52432096\n",
            "Epoch 9360 MSE = 0.52432096\n",
            "Epoch 9370 MSE = 0.52432096\n",
            "Epoch 9380 MSE = 0.52432096\n",
            "Epoch 9390 MSE = 0.52432096\n",
            "Epoch 9400 MSE = 0.52432096\n",
            "Epoch 9410 MSE = 0.52432096\n",
            "Epoch 9420 MSE = 0.52432096\n",
            "Epoch 9430 MSE = 0.52432096\n",
            "Epoch 9440 MSE = 0.52432096\n",
            "Epoch 9450 MSE = 0.52432096\n",
            "Epoch 9460 MSE = 0.52432096\n",
            "Epoch 9470 MSE = 0.52432096\n",
            "Epoch 9480 MSE = 0.52432096\n",
            "Epoch 9490 MSE = 0.52432096\n",
            "Epoch 9500 MSE = 0.52432096\n",
            "Epoch 9510 MSE = 0.52432096\n",
            "Epoch 9520 MSE = 0.52432096\n",
            "Epoch 9530 MSE = 0.52432096\n",
            "Epoch 9540 MSE = 0.52432096\n",
            "Epoch 9550 MSE = 0.52432096\n",
            "Epoch 9560 MSE = 0.52432096\n",
            "Epoch 9570 MSE = 0.52432096\n",
            "Epoch 9580 MSE = 0.52432096\n",
            "Epoch 9590 MSE = 0.52432096\n",
            "Epoch 9600 MSE = 0.52432096\n",
            "Epoch 9610 MSE = 0.52432096\n",
            "Epoch 9620 MSE = 0.52432096\n",
            "Epoch 9630 MSE = 0.52432096\n",
            "Epoch 9640 MSE = 0.52432096\n",
            "Epoch 9650 MSE = 0.52432096\n",
            "Epoch 9660 MSE = 0.52432096\n",
            "Epoch 9670 MSE = 0.52432096\n",
            "Epoch 9680 MSE = 0.52432096\n",
            "Epoch 9690 MSE = 0.52432096\n",
            "Epoch 9700 MSE = 0.52432096\n",
            "Epoch 9710 MSE = 0.52432096\n",
            "Epoch 9720 MSE = 0.52432096\n",
            "Epoch 9730 MSE = 0.52432096\n",
            "Epoch 9740 MSE = 0.52432096\n",
            "Epoch 9750 MSE = 0.52432096\n",
            "Epoch 9760 MSE = 0.52432096\n",
            "Epoch 9770 MSE = 0.52432096\n",
            "Epoch 9780 MSE = 0.52432096\n",
            "Epoch 9790 MSE = 0.52432096\n",
            "Epoch 9800 MSE = 0.52432096\n",
            "Epoch 9810 MSE = 0.52432096\n",
            "Epoch 9820 MSE = 0.52432096\n",
            "Epoch 9830 MSE = 0.52432096\n",
            "Epoch 9840 MSE = 0.52432096\n",
            "Epoch 9850 MSE = 0.52432096\n",
            "Epoch 9860 MSE = 0.52432096\n",
            "Epoch 9870 MSE = 0.52432096\n",
            "Epoch 9880 MSE = 0.52432096\n",
            "Epoch 9890 MSE = 0.52432096\n",
            "Epoch 9900 MSE = 0.52432096\n",
            "Epoch 9910 MSE = 0.52432096\n",
            "Epoch 9920 MSE = 0.52432096\n",
            "Epoch 9930 MSE = 0.52432096\n",
            "Epoch 9940 MSE = 0.52432096\n",
            "Epoch 9950 MSE = 0.52432096\n",
            "Epoch 9960 MSE = 0.52432096\n",
            "Epoch 9970 MSE = 0.52432096\n",
            "Epoch 9980 MSE = 0.52432096\n",
            "Epoch 9990 MSE = 0.52432096\n",
            "[[ 2.0685577 ]\n",
            " [ 0.8296204 ]\n",
            " [ 0.11875191]\n",
            " [-0.2655287 ]\n",
            " [ 0.30569768]\n",
            " [-0.00450293]\n",
            " [-0.03932632]\n",
            " [-0.8998829 ]\n",
            " [-0.87053835]]\n",
            "[[ 2.0685577 ]\n",
            " [ 0.8296204 ]\n",
            " [ 0.11875191]\n",
            " [-0.2655287 ]\n",
            " [ 0.30569768]\n",
            " [-0.00450293]\n",
            " [-0.03932632]\n",
            " [-0.8998829 ]\n",
            " [-0.87053835]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
