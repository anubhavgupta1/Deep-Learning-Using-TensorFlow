{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOaRbmK2lqyOoHFBi7zqKos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Deep-Learning-Using-TensorFlow/blob/master/4_Training_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC6jCBi0eBOG"
      },
      "source": [
        "#**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLAs1yE64kZh",
        "outputId": "a3b98ee8-9332-4aaf-ba0a-92a92d0277ce"
      },
      "source": [
        "!ls\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/\")\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.23-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "'Capstone Project Submission .desktop'\t\t     images\t    model_ckps4\n",
            "'Capstone Project Submission  (Responses).desktop'   import\t    model_ckps5\n",
            "'Colab Notebooks'\t\t\t\t     model_ckps     model_ckps6\n",
            " data\t\t\t\t\t\t     model_ckps1    model_ckps7\n",
            " Dataset\t\t\t\t\t     model_ckps10   model_ckps8\n",
            " drive\t\t\t\t\t\t     model_ckps2    model_ckps9\n",
            "'Getting started'\t\t\t\t     model_ckps3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NGp9k_-EHjsJ",
        "outputId": "d752b694-d484-46de-d8d8-c1c225ce024a"
      },
      "source": [
        "!pip install tensorflow==1.10.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/e6/a6d371306c23c2b01cd2cb38909673d17ddd388d9e4b3c0f6602bfd972c8/tensorflow-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4MB 74kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.33.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.10.0)\n",
            "Collecting tensorboard<1.11.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.3.3)\n",
            "Collecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 34.1MB/s \n",
            "\u001b[?25hCollecting numpy<=1.14.5,>=1.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 23.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.4.0)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.4 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.17.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.1 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorboard, setuptools, tensorflow\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed numpy-1.14.5 setuptools-39.1.0 tensorboard-1.10.0 tensorflow-1.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqLgCrtT4qOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dac5785-5dec-4ce3-906f-3a30530390d4"
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from matplotlib.colors import ListedColormap\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8T4M6N94xpT"
      },
      "source": [
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deep\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQfDEFLkXs_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39f0fe4-88a6-4696-aca2-ec29aae0f5e0"
      },
      "source": [
        "mnist = input_data.read_data_sets(\"data/\")\n",
        "X_train = mnist.train.images\n",
        "X_test = mnist.test.images\n",
        "y_train = mnist.train.labels.astype(\"int\")\n",
        "y_test = mnist.test.labels.astype(\"int\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-36e6f08442f0>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUsp70wqXeRR"
      },
      "source": [
        "#**Reusing a TensorFlow Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imXbEp55QXMs"
      },
      "source": [
        "reset_graph()\n",
        "for op in tf.get_default_graph().get_operations():\n",
        "    print(op.name)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUUNPF9OUwJu",
        "outputId": "337824ed-9d39-45f0-d1ef-f13c28a4be11"
      },
      "source": [
        "tf.get_default_graph().get_operations()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g70B38afU2kK"
      },
      "source": [
        "# Reusing a TensorFlow Model - Step 1\n",
        "\n",
        "reset_graph()\n",
        "saver = tf.train.import_meta_graph(\"model_ckps8/my_model_final.ckpt.meta\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf2O2cK2U-89",
        "outputId": "74312d7a-d868-4e8e-823e-92b61dfe9355"
      },
      "source": [
        "# Reusing a TensorFlow Model - Step 2\n",
        "# List all the operations using below code\n",
        "\n",
        "for op in tf.get_default_graph().get_operations():\n",
        "    print(op.name)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "y\n",
            "hidden1/kernel/Initializer/truncated_normal/shape\n",
            "hidden1/kernel/Initializer/truncated_normal/mean\n",
            "hidden1/kernel/Initializer/truncated_normal/stddev\n",
            "hidden1/kernel/Initializer/truncated_normal/TruncatedNormal\n",
            "hidden1/kernel/Initializer/truncated_normal/mul\n",
            "hidden1/kernel/Initializer/truncated_normal\n",
            "hidden1/kernel\n",
            "hidden1/kernel/Assign\n",
            "hidden1/kernel/read\n",
            "hidden1/bias/Initializer/zeros\n",
            "hidden1/bias\n",
            "hidden1/bias/Assign\n",
            "hidden1/bias/read\n",
            "dnn/hidden1/MatMul\n",
            "dnn/hidden1/BiasAdd\n",
            "dnn/hidden1/Selu\n",
            "hidden2/kernel/Initializer/truncated_normal/shape\n",
            "hidden2/kernel/Initializer/truncated_normal/mean\n",
            "hidden2/kernel/Initializer/truncated_normal/stddev\n",
            "hidden2/kernel/Initializer/truncated_normal/TruncatedNormal\n",
            "hidden2/kernel/Initializer/truncated_normal/mul\n",
            "hidden2/kernel/Initializer/truncated_normal\n",
            "hidden2/kernel\n",
            "hidden2/kernel/Assign\n",
            "hidden2/kernel/read\n",
            "hidden2/bias/Initializer/zeros\n",
            "hidden2/bias\n",
            "hidden2/bias/Assign\n",
            "hidden2/bias/read\n",
            "dnn/hidden2/MatMul\n",
            "dnn/hidden2/BiasAdd\n",
            "dnn/hidden2/Selu\n",
            "hidden3/kernel/Initializer/truncated_normal/shape\n",
            "hidden3/kernel/Initializer/truncated_normal/mean\n",
            "hidden3/kernel/Initializer/truncated_normal/stddev\n",
            "hidden3/kernel/Initializer/truncated_normal/TruncatedNormal\n",
            "hidden3/kernel/Initializer/truncated_normal/mul\n",
            "hidden3/kernel/Initializer/truncated_normal\n",
            "hidden3/kernel\n",
            "hidden3/kernel/Assign\n",
            "hidden3/kernel/read\n",
            "hidden3/bias/Initializer/zeros\n",
            "hidden3/bias\n",
            "hidden3/bias/Assign\n",
            "hidden3/bias/read\n",
            "dnn/hidden3/MatMul\n",
            "dnn/hidden3/BiasAdd\n",
            "dnn/hidden3/Selu\n",
            "hidden4/kernel/Initializer/truncated_normal/shape\n",
            "hidden4/kernel/Initializer/truncated_normal/mean\n",
            "hidden4/kernel/Initializer/truncated_normal/stddev\n",
            "hidden4/kernel/Initializer/truncated_normal/TruncatedNormal\n",
            "hidden4/kernel/Initializer/truncated_normal/mul\n",
            "hidden4/kernel/Initializer/truncated_normal\n",
            "hidden4/kernel\n",
            "hidden4/kernel/Assign\n",
            "hidden4/kernel/read\n",
            "hidden4/bias/Initializer/zeros\n",
            "hidden4/bias\n",
            "hidden4/bias/Assign\n",
            "hidden4/bias/read\n",
            "dnn/hidden4/MatMul\n",
            "dnn/hidden4/BiasAdd\n",
            "dnn/hidden4/Selu\n",
            "hidden5/kernel/Initializer/truncated_normal/shape\n",
            "hidden5/kernel/Initializer/truncated_normal/mean\n",
            "hidden5/kernel/Initializer/truncated_normal/stddev\n",
            "hidden5/kernel/Initializer/truncated_normal/TruncatedNormal\n",
            "hidden5/kernel/Initializer/truncated_normal/mul\n",
            "hidden5/kernel/Initializer/truncated_normal\n",
            "hidden5/kernel\n",
            "hidden5/kernel/Assign\n",
            "hidden5/kernel/read\n",
            "hidden5/bias/Initializer/zeros\n",
            "hidden5/bias\n",
            "hidden5/bias/Assign\n",
            "hidden5/bias/read\n",
            "dnn/hidden5/MatMul\n",
            "dnn/hidden5/BiasAdd\n",
            "dnn/hidden5/Selu\n",
            "outputs/kernel/Initializer/random_uniform/shape\n",
            "outputs/kernel/Initializer/random_uniform/min\n",
            "outputs/kernel/Initializer/random_uniform/max\n",
            "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
            "outputs/kernel/Initializer/random_uniform/sub\n",
            "outputs/kernel/Initializer/random_uniform/mul\n",
            "outputs/kernel/Initializer/random_uniform\n",
            "outputs/kernel\n",
            "outputs/kernel/Assign\n",
            "outputs/kernel/read\n",
            "outputs/bias/Initializer/zeros\n",
            "outputs/bias\n",
            "outputs/bias/Assign\n",
            "outputs/bias/read\n",
            "dnn/outputs/MatMul\n",
            "dnn/outputs/BiasAdd\n",
            "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
            "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
            "loss/Const\n",
            "loss/loss\n",
            "gradients/Shape\n",
            "gradients/grad_ys_0\n",
            "gradients/Fill\n",
            "gradients/loss/loss_grad/Reshape/shape\n",
            "gradients/loss/loss_grad/Reshape\n",
            "gradients/loss/loss_grad/Shape\n",
            "gradients/loss/loss_grad/Tile\n",
            "gradients/loss/loss_grad/Shape_1\n",
            "gradients/loss/loss_grad/Shape_2\n",
            "gradients/loss/loss_grad/Const\n",
            "gradients/loss/loss_grad/Prod\n",
            "gradients/loss/loss_grad/Const_1\n",
            "gradients/loss/loss_grad/Prod_1\n",
            "gradients/loss/loss_grad/Maximum/y\n",
            "gradients/loss/loss_grad/Maximum\n",
            "gradients/loss/loss_grad/floordiv\n",
            "gradients/loss/loss_grad/Cast\n",
            "gradients/loss/loss_grad/truediv\n",
            "gradients/zeros_like\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
            "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
            "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/outputs/MatMul_grad/MatMul\n",
            "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
            "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden5/Selu_grad/SeluGrad\n",
            "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden4/Selu_grad/SeluGrad\n",
            "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden3/Selu_grad/SeluGrad\n",
            "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden2/Selu_grad/SeluGrad\n",
            "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden1/Selu_grad/SeluGrad\n",
            "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
            "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
            "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
            "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
            "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
            "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
            "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
            "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
            "clip_by_value/Minimum/y\n",
            "clip_by_value/Minimum\n",
            "clip_by_value/y\n",
            "clip_by_value\n",
            "clip_by_value_1/Minimum/y\n",
            "clip_by_value_1/Minimum\n",
            "clip_by_value_1/y\n",
            "clip_by_value_1\n",
            "clip_by_value_2/Minimum/y\n",
            "clip_by_value_2/Minimum\n",
            "clip_by_value_2/y\n",
            "clip_by_value_2\n",
            "clip_by_value_3/Minimum/y\n",
            "clip_by_value_3/Minimum\n",
            "clip_by_value_3/y\n",
            "clip_by_value_3\n",
            "clip_by_value_4/Minimum/y\n",
            "clip_by_value_4/Minimum\n",
            "clip_by_value_4/y\n",
            "clip_by_value_4\n",
            "clip_by_value_5/Minimum/y\n",
            "clip_by_value_5/Minimum\n",
            "clip_by_value_5/y\n",
            "clip_by_value_5\n",
            "clip_by_value_6/Minimum/y\n",
            "clip_by_value_6/Minimum\n",
            "clip_by_value_6/y\n",
            "clip_by_value_6\n",
            "clip_by_value_7/Minimum/y\n",
            "clip_by_value_7/Minimum\n",
            "clip_by_value_7/y\n",
            "clip_by_value_7\n",
            "clip_by_value_8/Minimum/y\n",
            "clip_by_value_8/Minimum\n",
            "clip_by_value_8/y\n",
            "clip_by_value_8\n",
            "clip_by_value_9/Minimum/y\n",
            "clip_by_value_9/Minimum\n",
            "clip_by_value_9/y\n",
            "clip_by_value_9\n",
            "clip_by_value_10/Minimum/y\n",
            "clip_by_value_10/Minimum\n",
            "clip_by_value_10/y\n",
            "clip_by_value_10\n",
            "clip_by_value_11/Minimum/y\n",
            "clip_by_value_11/Minimum\n",
            "clip_by_value_11/y\n",
            "clip_by_value_11\n",
            "GradientDescent/learning_rate\n",
            "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
            "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
            "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
            "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
            "GradientDescent\n",
            "eval/in_top_k/InTopKV2/k\n",
            "eval/in_top_k/InTopKV2\n",
            "eval/Cast\n",
            "eval/Const\n",
            "eval/accuracy\n",
            "init\n",
            "save/Const\n",
            "save/SaveV2/tensor_names\n",
            "save/SaveV2/shape_and_slices\n",
            "save/SaveV2\n",
            "save/control_dependency\n",
            "save/RestoreV2/tensor_names\n",
            "save/RestoreV2/shape_and_slices\n",
            "save/RestoreV2\n",
            "save/Assign\n",
            "save/Assign_1\n",
            "save/Assign_2\n",
            "save/Assign_3\n",
            "save/Assign_4\n",
            "save/Assign_5\n",
            "save/Assign_6\n",
            "save/Assign_7\n",
            "save/Assign_8\n",
            "save/Assign_9\n",
            "save/Assign_10\n",
            "save/Assign_11\n",
            "save/restore_all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0Wq2_ZhVEPR"
      },
      "source": [
        "# Reusing a TensorFlow Model - Step 3\n",
        "# Once we know which operations do we need then\n",
        "# we can get a handle on them using the graph's get_operation_by_name() or get_tensor_by_name() methods\n",
        "\n",
        "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
        "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-IORc_tVHYk"
      },
      "source": [
        "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
        "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs8Ue2bjVL_3",
        "outputId": "8af62157-b618-488b-c964-c84c11ee0e75"
      },
      "source": [
        "graph = tf.get_default_graph()\n",
        "graph"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.framework.ops.Graph at 0x7f1c173b2400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONJ3wQCvW8Xo"
      },
      "source": [
        "n_epochs = 5\n",
        "batch_size = 200"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYA8mC9BWbDY",
        "outputId": "c88bdf52-7373-4446-de5f-ca75a8adc62d"
      },
      "source": [
        "# Reusing a TensorFlow Model - Step 4\n",
        "# Now you can start a session, restore the model's state and continue training on your data\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"model_ckps8/my_model_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,y: mnist.test.labels})\n",
        "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"model_ckps9/my_new_model_final.ckpt\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model_ckps8/my_model_final.ckpt\n",
            "0 Test accuracy: 0.9619\n",
            "1 Test accuracy: 0.9643\n",
            "2 Test accuracy: 0.964\n",
            "3 Test accuracy: 0.9623\n",
            "4 Test accuracy: 0.9639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5i-7gfHXjDt"
      },
      "source": [
        "#**Reusing only part of the original model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENgRw85IXosn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbb5b93-1adf-4f16-f0f4-b2bb0b85b197"
      },
      "source": [
        "# In general, we restore only early layers\n",
        "# Let's restore only hidden layers 1, 2 and 3\n",
        "\n",
        "\n",
        "#find out variables to reuse\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"hidden[123]\") # regular expression\n",
        "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
        "reuse_vars_dict"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hidden1/bias': <tf.Variable 'hidden1/bias:0' shape=(300,) dtype=float32_ref>,\n",
              " 'hidden1/kernel': <tf.Variable 'hidden1/kernel:0' shape=(784, 300) dtype=float32_ref>,\n",
              " 'hidden2/bias': <tf.Variable 'hidden2/bias:0' shape=(50,) dtype=float32_ref>,\n",
              " 'hidden2/kernel': <tf.Variable 'hidden2/kernel:0' shape=(300, 50) dtype=float32_ref>,\n",
              " 'hidden3/bias': <tf.Variable 'hidden3/bias:0' shape=(50,) dtype=float32_ref>,\n",
              " 'hidden3/kernel': <tf.Variable 'hidden3/kernel:0' shape=(50, 50) dtype=float32_ref>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WlYLGTmehbm"
      },
      "source": [
        "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u4uKCoHdaGY"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa5g9xyzbH4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04dd2b39-fc7f-453e-f550-0260ab940c8e"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"model_ckps8/my_model_final.ckpt\") #restoring\n",
        "\n",
        "    for epoch in range(n_epochs):                                     \n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,y: mnist.test.labels})\n",
        "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"model_ckps10/my_new_model_final.ckpt\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model_ckps8/my_model_final.ckpt\n",
            "0 Test accuracy: 0.9517\n",
            "1 Test accuracy: 0.9541\n",
            "2 Test accuracy: 0.9577\n",
            "3 Test accuracy: 0.9578\n",
            "4 Test accuracy: 0.9596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT2LsajXpBp5"
      },
      "source": [
        "#**Reusing Models from Other Frameworks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R5ypIZ3bozm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715aa6b3-e0a7-47ac-a481-b8b51af0c2b8"
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden1 = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
        "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "# [...] Build the rest of the model\n",
        "\n",
        "# Get a handle on the assignment nodes for the hidden1 variables\n",
        "graph = tf.get_default_graph()\n",
        "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
        "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
        "init_kernel = assign_kernel.inputs[1]\n",
        "init_bias = assign_bias.inputs[1]\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
        "    # [...] Train the model on your new task\n",
        "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7bgbJl-bwDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ffcfb8-33b2-414f-cd59-000f40c86143"
      },
      "source": [
        "graph = tf.get_default_graph()\n",
        "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
        "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
        "for u in assign_kernel.inputs:\n",
        "    print(u)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"hidden1/kernel:0\", shape=(2, 3), dtype=float32_ref)\n",
            "Tensor(\"hidden1/kernel/Initializer/random_uniform:0\", shape=(2, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}