{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNW6a3pVhrn9iMn4cEfawDV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Deep-Learning-Using-TensorFlow/blob/master/6_Training_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC6jCBi0eBOG"
      },
      "source": [
        "#**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLAs1yE64kZh",
        "outputId": "d7d1326e-7850-4c70-cdc7-ff10ee32e214"
      },
      "source": [
        "!ls\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/\")\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.23-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "'Capstone Project Submission .desktop'\n",
            "'Capstone Project Submission  (Responses).desktop'\n",
            "'Colab Notebooks'\n",
            " data\n",
            " Dataset\n",
            " drive\n",
            "'Getting started'\n",
            " images\n",
            " import\n",
            " model_ckps\n",
            " model_ckps1\n",
            " model_ckps10\n",
            " model_ckps2\n",
            " model_ckps3\n",
            " model_ckps4\n",
            " model_ckps5\n",
            " model_ckps6\n",
            " model_ckps7\n",
            " model_ckps8\n",
            " model_ckps9\n",
            " model_ckps_adagrad\n",
            " model_ckps_adam\n",
            " model_ckps_Learning_rate_scheduling\n",
            " model_ckps_momentum\n",
            " model_ckps_nestrov\n",
            " model_ckps_normal_gd\n",
            " model_ckps_rms_prop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NGp9k_-EHjsJ",
        "outputId": "76b05c30-cc76-40d5-d1d5-e91f6580adc8"
      },
      "source": [
        "!pip install tensorflow==1.10.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/e6/a6d371306c23c2b01cd2cb38909673d17ddd388d9e4b3c0f6602bfd972c8/tensorflow-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4MB 79kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.35.1)\n",
            "Collecting numpy<=1.14.5,>=1.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 33.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.15.0)\n",
            "Collecting tensorboard<1.11.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.33.2)\n",
            "Collecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.0) (3.4.0)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.4 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.17.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.1 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorboard, setuptools, tensorflow\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed numpy-1.14.5 setuptools-39.1.0 tensorboard-1.10.0 tensorflow-1.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqLgCrtT4qOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c8692f-d028-4d9a-d468-a0326eee0022"
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from matplotlib.colors import ListedColormap\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8T4M6N94xpT"
      },
      "source": [
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deep\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQfDEFLkXs_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c597f131-446d-4488-96e0-a572abad0a2c"
      },
      "source": [
        "mnist = input_data.read_data_sets(\"data/\")\n",
        "X_train = mnist.train.images\n",
        "X_test = mnist.test.images\n",
        "y_train = mnist.train.labels.astype(\"int\")\n",
        "y_test = mnist.test.labels.astype(\"int\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-36e6f08442f0>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQffUr1p959R"
      },
      "source": [
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_outputs = 10"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNlQXMuZY2du"
      },
      "source": [
        "n_epochs = 20\n",
        "batch_size = 200"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GmPGONwWLQ9"
      },
      "source": [
        "#**L1 Regularization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdyhKOjqWTw4"
      },
      "source": [
        "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_o66aO89uDO"
      },
      "source": [
        "# Let's implement ℓ1 regularization manually.\n",
        "# Assuming we have just one hidden layer\n",
        "\n",
        "reset_graph()\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y1BSjME98EG"
      },
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An33x6yfWctT"
      },
      "source": [
        "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5Ewji_t-KD_"
      },
      "source": [
        "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
        "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
        "\n",
        "scale = 0.001 # l1 regularization hyperparameter"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQdrzhIu-Nw3"
      },
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
        "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
        "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
        "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Pi8SBIWjc0"
      },
      "source": [
        "The rest is just as usual:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7brH6HoOJzH"
      },
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5ICnC6dYat9"
      },
      "source": [
        "learning_rate = 0.01"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhBLtyhkYXol"
      },
      "source": [
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rL1iNxmYVij"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j9bIbf2OPfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9609ed5d-20c5-43e1-ce6b-f77be4323246"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,y: mnist.test.labels})\n",
        "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"model_ckps_reg_1/my_model_final.ckpt\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Test accuracy: 0.8347\n",
            "1 Test accuracy: 0.8713\n",
            "2 Test accuracy: 0.8832\n",
            "3 Test accuracy: 0.8907\n",
            "4 Test accuracy: 0.8959\n",
            "5 Test accuracy: 0.8978\n",
            "6 Test accuracy: 0.9015\n",
            "7 Test accuracy: 0.9035\n",
            "8 Test accuracy: 0.9033\n",
            "9 Test accuracy: 0.9064\n",
            "10 Test accuracy: 0.906\n",
            "11 Test accuracy: 0.907\n",
            "12 Test accuracy: 0.9076\n",
            "13 Test accuracy: 0.9068\n",
            "14 Test accuracy: 0.9065\n",
            "15 Test accuracy: 0.9076\n",
            "16 Test accuracy: 0.9068\n",
            "17 Test accuracy: 0.9063\n",
            "18 Test accuracy: 0.9064\n",
            "19 Test accuracy: 0.9065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbo5KdpqW3LO"
      },
      "source": [
        "#**Applying ℓ1 regularization in the network having two hidden layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGNKijv4W7nh"
      },
      "source": [
        "Alternatively, we can pass a regularization function to the tf.layers.dense() function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC9LUlsbXQ9c"
      },
      "source": [
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm6EqAHfW6eP"
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-VMSIqgX2sU"
      },
      "source": [
        "Next, we will use Python's partial() function to avoid repeating the same arguments over and over again. Note that we set the kernel_regularizer argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Gie5dTXeQ8"
      },
      "source": [
        "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.selu, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNipnax0Xlen"
      },
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
        "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
        "    logits = my_dense_layer(hidden2, n_outputs, activation=None,name=\"outputs\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl_XAuA2YArq"
      },
      "source": [
        "Next we must add the regularization losses to the base loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_MPii0JYB8z"
      },
      "source": [
        "with tf.name_scope(\"loss\"):                                     \n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4iLaDmIYL_R"
      },
      "source": [
        "And the rest is the same as usual:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s61owbUrYP_9"
      },
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RtZZQlKYkSd"
      },
      "source": [
        "learning_rate = 0.01"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qTUUu-UYhIP"
      },
      "source": [
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i9pb3MbYeue"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05nDVF9jYszh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b4ff0c-64bb-44ea-ff95-ec76327e2135"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,y: mnist.test.labels})\n",
        "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"model_ckps_reg_2/my_model_final.ckpt\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Test accuracy: 0.8799\n",
            "1 Test accuracy: 0.8977\n",
            "2 Test accuracy: 0.9062\n",
            "3 Test accuracy: 0.9106\n",
            "4 Test accuracy: 0.9144\n",
            "5 Test accuracy: 0.9161\n",
            "6 Test accuracy: 0.9193\n",
            "7 Test accuracy: 0.9209\n",
            "8 Test accuracy: 0.9229\n",
            "9 Test accuracy: 0.9249\n",
            "10 Test accuracy: 0.9272\n",
            "11 Test accuracy: 0.9296\n",
            "12 Test accuracy: 0.9288\n",
            "13 Test accuracy: 0.9319\n",
            "14 Test accuracy: 0.9318\n",
            "15 Test accuracy: 0.9332\n",
            "16 Test accuracy: 0.9346\n",
            "17 Test accuracy: 0.936\n",
            "18 Test accuracy: 0.9367\n",
            "19 Test accuracy: 0.9378\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}